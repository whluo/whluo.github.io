<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Wenhan Luo</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenhan Luo</name>
              </p>
            

             <p style="text-align:center">
                <a href="mailto:whluo.china@gmail.com"><i class="fa fa-envelope-o fa-fw" aria-hidden="true" style="font-size:100%;width:4ch;margin-top:1px;"></i>Email</a> &nbsp &nbsp  &nbsp 
                <a href="https://scholar.google.com.hk/citations?user=g20Q12MAAAAJ&hl=en"><i class="fa fa-graduation-cap fa-fw" style="font-size:100%;width:4.2ch;margin-top:1px;"></i>Scholar</a> &nbsp &nbsp  &nbsp 
                <a href="publication.html"><i class="fa fa-file-o fa-fw" style="font-size:100%;width:4ch;margin-top:1px;"></i>Publication</a> &nbsp &nbsp  &nbsp 
               <a href="link.html"><i class="fa fa-external-link-square fa-fw" style="font-size:100%;width:4ch;margin-top:1px;"></i>Links</a>
              </p>
            </td>
          </tr>
        </tbody></table>



        <div class="section">
          
          <div class="mysection">
            <p style="text-align:justify; text-justify:inter-ideograph;">I am an applied research scientist at Tencent, where I work on solving real-world problems using computer vision and machine learning techniques.
              Prior to Tencent, I worked for Amazon (A9) in Palo Alto, California, where I developed deep models for better visual search experience.
              Before that, I worked as a research scientist in Tencent AI Lab. The techniques I have developed/involved have been shipped to several products in Tencent such as WeChat, Tencent Video, and myapp. I received the Ph.D. degree from Imperial College London, UK, 2016, M.E. degree from Institute of Automation, Chinese Academy of Sciences, China, 2012 and B.E. degree from Huazhong University of Science and Technology, China, 2009. [<a href="data/WenhanLuo_CV.pdf">CV</a>]</p>
          
          
               
           </div>
          </div>

          <br><br>

        <div class="section">
          <h2>Research</h2>
            <div class="research">
              <p style="text-align:justify; text-justify:inter-ideograph;">I am interested in several topics in computer vision and machine learning. Specifically, my current research focuses on creative AI, such as image/video synthesis.</p>
            </div>
        </div>

        <br><br>

        <div class="section" id="news">
          <h2>Updates</h2>
            <div class="paper">
            <p>2022/06 - Paper accepted by ACM MM 2022 (multi-object tracking).</p>
            <p>2022/06 - Paper accepted by KBS (generative model for facial expression recognition).</p>
            <p>2022/05 - TPAMI paper accepted (face hallucination).</p>
            <p>2022/05 - Paper accepted by IJCV (image deblurring survey).</p>
            <p>2022/04 - Paper accepted by IJCV (image deraining).</p>
            <p>2022/03 - Paper accepted by CVPR2022 (aesthetic text logo synthesis).</p>
            <p>2022/01 - TPAMI paper accepted (video deraining).</p>
            <p>2021/09 - TIP paper accepted (image deraining).</p>
            <p>2021/08 - Invited to serve as a Senior PC member for AAAI 2022.</p> 
            <p>2021/08 - TIP paper accepted (image desnow).</p>
            <p>2021/07 - Paper accepted by ICCV2021 (image SR benchmarking).</p> 
            <p>2021/07 - One paper (image deblur & SR) to appear in IEEE Transactions on Image Processing.</p> 
            <p>2021/06 - TMM paper accepted (action recognition).</p>
            <p class="hidden_news" style="display:none">2021/05 - Our work of active visual tracking is accepted by ICML2021.</p>
            <p class="hidden_news" style="display:none">2021/05 - One paper of image dehazing to appear in IEEE Transactions on Image Processing.</p>
            <p class="hidden_news" style="display:none">2021/04 - Our work of human image synthesis is accepted to appear in TPAMI.</p>   
            <p class="hidden_news" style="display:none">2021/03 - One paper to appear in IEEE Transactions on Geoscience and Remote Sensing.</p> 
            <p class="hidden_news" style="display:none">2021/02 - One paper to appear in IEEE Transactions on Multimedia.</p>  
            <p class="hidden_news" style="display:none">2020/12 - The paper "Multiple Object Tracking: A Literature Review" is accepted by Artificial Intelligence.</p>
            <p class="hidden_news" style="display:none">2020/12 - Invited to serve as a Senior PC member for IJCAI 2021.</p>
            <p class="hidden_news" style="display:none">2020/11 - One paper of pedestrian detection to appear in IEEE Transactions on Image Processing.</p>
            <p class="hidden_news" style="display:none">2020/09 - An invited talk is given in SUSTech, hosted by <a href="https://eee.sustc.edu.cn/?view=%E5%94%90%E6%99%93%E9%A2%96&jsid=18&lang=en">Prof. Xiaoying Tang</a>.</p>
            <p class="hidden_news" style="display:none">2020/09 - One paper of optical flow estimation to appear in IEEE Transactions on Image Processing.</p>
            <p class="hidden_news" style="display:none">2020/07 - One paper to appear in ACM MM 2020 (Oral).</p>
            <p class="hidden_news" style="display:none">2020/07 - One paper to appear in ECCV2020.</p>
            <p class="hidden_news" style="display:none">2020/06 - One paper of multiple object tracking to appear in Pattern Recognition.</p>
            <p class="hidden_news" style="text-align:justify; display:none; text-justify:inter-ideograph;">2020/05 - We are organizing a special issue of action recognition and detection on CVIU. Submission deadline is Sep 15th. See the <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding/call-for-papers/modeling-methodology-and-applications-of-action-recognition">CFP</a> if interested</p>
            <p class="hidden_news" style="display:none">2020/02 - Two papers (one oral + one poster) to appear in CVPR2020.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/10 - One paper to appear in TPAMI, entitled "AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking".</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/09 - Code and dataset of our ICCV2019 paper for motion imitation, appearance transfer and novel view synthesis are released. Check the project page <a href="https://svip-lab.github.io/project/impersonator">here</a>.</p>
            <p class="hidden_news" style="display:none">2019/09 - One paper to appear in IJCV.</p>
            <p class="hidden_news" style="display:none">2019/07 - One paper to appear in ICCV2019.</p>
            <p class="hidden_news" style="display:none">2019/07 - The code of our ACL2019 paper is released. Check it <a href="https://github.com/zfchenUnique/WSSTG">here</a>.</p>
            <p class="hidden_news" style="display:none">2019/07 - The code of our ICLR2019 paper is released. Check it <a href="https://github.com/zfw1226/active_tracking_rl">here</a>.</p>
            <p class="hidden_news" style="display:none">2019/05 - Join Amazon in California as a research scientist.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/06 - Our CVPR paper entitled "Learning to Compose Dynamic Tree Structures for Visual Context" is selected as one of the best paper finalists (50 out of the 1294 accepted papers in CVPR2019).</p>
            <p class="hidden_news" style="display:none">2019/05 - One paper of video grounding to appear in ACL2019 as a long paper, and oral presentation.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/05 - Serve as program committee member of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drones 2019: A Challenge</a> in conjunction with ICCV2019.</p>
            <p class="hidden_news" style="display:none">2019/03 - Four papers (2 orals + 2 posters) to appear in CVPR2019.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/02 - Serve as program committee member of the <a href="https://motchallenge.net/workshops/bmtt2019/index.html">4th BMTT MOT Challenge Workshop</a> and the <a href="https://reid-mct.github.io/2019/">2nd Workshop and Challenge on Target Re-identification and Multi-Target Multi-Camera Tracking</a> in conjunction with CVPR2019.</p>
            <p class="hidden_news" style="display:none">2019/02 - The code of our CVPR 2018 paper is released. Check it <a href="https://github.com/weixiong-ur/mdgan">here</a>.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2019/01 - Our work of "<a href="https://arxiv.org/abs/1808.03405">End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning</a>" is accepted by TPAMI.</p>
            <p class="hidden_news" style="display:none">2018/12 - One paper to appear in ICLR2019. Congratulations to Fangwei Zhong.</p>
            <p class="hidden_news" style="display:none">2018/11 - One paper to appear in <a href="https://sites.google.com/view/deep-rl-workshop-nips-2018/home">NIPS2018 workshop on Deep Reinforcement Learning</a>.</p>

            <p class="hidden_news" style="display:none">2018/11 - One paper to appear in AAAI2019. Congratulations to Kaihao Zhang.</p>
            <p class="hidden_news" style="display:none">2018/10 - Welcome Tianrui Liu (Imperial College London) on board as a research intern.</p>

            
            <p class="hidden_news" style="display:none">2018/09 - The code of our ICML2018 paper is released. Check it <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018">here</a>.</p>
           
            <p class="hidden_news" style="display:none">2018/09 - The code of our ECCV2018 paper "Bi-Real Net" is released. Check it <a href="https://github.com/liuzechun/Bi-Real-net">here</a>.</p>
            <p class="hidden_news" style="display:none">2018/08 - Our work of video deblur is accepted by IEEE Transactions on Image Processing. Congratulations to Kaihao Zhang.</p>
            <p class="hidden_news" style="display:none">2018/08 - One paper of multiple object tracking is accepted by IEEE Transactions on Image Processing.</p>

            
            <p class="hidden_news" style="display:none">2018/07 - The dataset for our CVPR2018 paper, Sky Scene is released. See our <a href="https://sites.google.com/site/whluoimperial/mdgan">project page</a> for details.</p>
            <p class="hidden_news" style="display:none">2018/07 - One paper to appear in ECCV2018. Congratulations to Zechun Liu.</p>

       
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2018/05 - Our work of End-to-end Active Object Tracking via Reinforcement Learning is accepted by ICML2018. The camera-ready version will come soon.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2018/05 - Serve as a member of the advisory committee of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drone: A Challenge</a> (VisDrone2018, for short) in conjunction with ECCV2018.</p>
            <p class="hidden_news" style="display:none">2018/04 - Welcome Jia Wan (NWPU) on board as an intern.</p>
            <p class="hidden_news" style="display:none">2018/02 - One paper to appear in CVPR2018. Congratulations to Wei Xiong.</p>
            <p class="hidden_news" style="display:none">2018/01 - Welcome our intern Zechun Liu (HKUST) on board.</p>
            <p class="hidden_news" style="display:none">2017/10 - Welcome Yiming Chen (Imperial College London) on board as an intern in Tencent AI Lab.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2017/08 - Welcome Kaihao Zhang on board as intern in Tencent AI Lab. Kaihao is from Australian National University and will work close with me for about six months.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2017/06 - Welcome Weiyue Su and Wei Xiong on board as intern in Tencent AI Lab. Weiyue is from South China University of Technology. Wei is from Wuhan University.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2017/05 - Serve as a program committee member of the First <a href="https://motchallenge.net/workshops/bmtt-pets2017/index.html">Joint BMPP-PETS Workshop on Tracking and Surveillance</a> in conjunction with CVPR2017.</p>
           
            <p class="hidden_news" style="display:none">2017/04 - One paper to appear in CVPR2017.</p>
            <p class="hidden_news" style="display:none">2016/07 - Join Tencent AI Lab as a research scientist.</p> 
            <p class="hidden_news" style="display:none">2016/06 - Pass viva exam and obtained the Ph.D. degree, examined by <a href="https://www.eecs.qmul.ac.uk/~txiang/">Tao (Tony) Xiang</a> from Queen Mary University of London.</p>
            <p class="hidden_news" style="text-align:justify; text-justify:inter-ideograph;display:none">2016/05 - Serve as a program committee member of the workshop <a href="https://motchallenge.net/workshops/bmtt2016/">Benchmarking Multi-Target Tracking: MOTChallenge</a> in conjunction with ECCV2016.</p>
            <p class="hidden_news" style="display:none">2015/02 - Start internship in Microsoft Research Asia with <a href="http://www.davidwipf.com/">Dr. David Wipf</a> (from Feb 2015 to June 2015).</p>
            
            
            
            <p id="ShowLink_news"><a href="#" onclick="var elements = document.getElementsByClassName('hidden_news');for(var i=0; i&lt;elements.length; i++) {elements[i].style.display='block';}document.getElementById('ShowLink_news').style.display='none';document.getElementById('HideLink_news').style.display='block';return false;" title="Show more News"><font size="2">Show more</font></a></p>
            <p id="HideLink_news" style="display:none"><a href="#" onclick="var elements = document.getElementsByClassName('hidden_news');for(var i=0; i&lt;elements.length; i++) {elements[i].style.display='none';}document.getElementById('HideLink_news').style.display='none';document.getElementById('ShowLink_news').style.display='block';return false;" title="Show less News"><font size="2">Show less</font></a></p>
          
            </div>
          </div>

         <br><br>
          
         <div class="section">
          <h2>Recent Projects</h2>
            <div class="research">



              <a href="https://wxiong.me/finegrain/"><papertitle>Fine-grained Image-to-Image Transformation towards Visual Recognition</papertitle></a>,
              <br>
              W. Xiong, Y. He, Y. Zhang, W. Luo, L. Ma, J. Luo,
              <br>
              <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2020.</i>

              <p style="text-align:justify; text-justify:inter-ideograph;">Image-to-image translation with fine-grained category towards maintaining identity information.</p>



              <br>
              <a href="https://svip-lab.github.io/project/impersonator.html"><papertitle>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis</papertitle></a>,
              <br>
              W. Liu+, Z. Piao, J. Min, W. Luo, L. Ma, S. Gao,
              <br>
              <i>Proc. of International Conference on Computer Vision (ICCV), Korea, 2019.</i>

              <p style="text-align:justify; text-justify:inter-ideograph;">A unified framework for human motion transfer, appearance transfer and novel view synthesis using GAN.</p>



              <br>
              <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"><papertitle>End-to-end Active Object Tracking via Reinforcement Learning</papertitle></a>,
              <br>W. Luo*, P. Sun*, F. Zhong, W. Liu, T. Zhang and Y. Wang,<br>
                          <i>International Conference on Machine Learning (ICML), Sweden, 2018.</i>
              <p style="text-align:justify; text-justify:inter-ideograph;">The first work proposing the active visual tracking problem and addressing it with reinforcement learning.</p>


              <br>
              <a href="https://sites.google.com/site/whluoimperial/mdgan"><papertitle>Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</papertitle></a>,
              <br>W. Xiong+, W. Luo, L. Ma, W. Liu and J. Luo,<br>
                            <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2018.</i>
              <p style="text-align:justify; text-justify:inter-ideograph;">A multi-stage GAN framework to generate time-lapse video given a single image.</p>


              





            </div>
        </div>






      <br><br>

        <div style="clear:both;">
          
          <p align="center"><font size="2"><a href="http://bjornstenger.github.io/">I like this website</a>  &nbsp &nbsp &nbsp &nbsp &nbsp   <a href="https://jonbarron.info/">Design credit</a></font></p><br />
        </div>


      </td>
    </tr>
  </table>
</body>

</html>
