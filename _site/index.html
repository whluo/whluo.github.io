<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Wenhan Luo - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Wenhan Luo">
<meta property="og:title" content="Wenhan Luo">


  <link rel="canonical" href="http://localhost:4000/">
  <meta property="og:url" content="http://localhost:4000/">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About</a></li>
          
            <li class="masthead__menu-item"><a href="/#research">Research</a></li>
          
            <li class="masthead__menu-item"><a href="/#updates">Updates</a></li>
          
            <li class="masthead__menu-item"><a href="assets/publication.html">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#experience">Experience</a></li>
          
            <li class="masthead__menu-item"><a href="assets/people.html">People</a></li>
          
            <li class="masthead__menu-item"><a href="assets/teaching.html">Teaching</a></li>
          
            <li class="masthead__menu-item"><a href="assets/services.html">Services</a></li>
          
            <li class="masthead__menu-item"><a href="assets/join_us.html">Join Us</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/wenhan_square.png" class="author__avatar" alt="Wenhan Luo">
  </div>

  <div class="author__content">
    <h3 class="author__name">Wenhan Luo</h3>
    <p class="author__bio">Associate Professor, HKUST</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;"></div></li>
      
      
        <li style="display: flex; align-items: center;">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true" style="margin-right: 8px; align-self: flex-start; transform: translateY(50%);"></i> 
          <span style="white-space: normal;">
            Clear Water Bay, Kowloon, Hong Kong
          </span>
        </li>
      
      
      
      
        <li><a href="mailto:whluo@ust.hk"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
        <li><a href="https://www.linkedin.com/in/wenhan-luo-a1843480"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
        <li><a href="https://dblp.org/pid/64/9877.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i> DBLP</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=g20Q12MAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:whluo@ust.hk"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
        <a href="https://www.linkedin.com/in/wenhan-luo-a1843480"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
        <a href="https://dblp.org/pid/64/9877.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com.hk/citations?user=g20Q12MAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<script type="text/javascript">
	$(document).ready(function() {
		$('a[href^="http"]').each(function() {
			$(this).attr('target', '_blank');
		});
	});
</script>

<script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<p style="text-align:justify; text-justify:inter-ideograph;"> I am now an Associate Professor with the Hong Kong University of Science and Technology (HKUST). Previously, I worked as an Associate Professor at <a href="https://en.wikipedia.org/wiki/Sun_Yat-sen_University" target="_blank">Sun Yat-sen University</a>, and an applied research scientist at <a href="https://en.wikipedia.org/wiki/Tencent" target="_blank">Tencent</a>, solving real-world problems using computer vision and machine learning techniques. Prior to Tencent, I worked for Amazon in Palo Alto, California, where I developed deep models for better visual search experience. Before that, I worked as a research scientist in Tencent AI Lab. The techniques I have developed/involved have been shipped to several products in Tencent such as WeChat, QQ, Tencent Video, Tencent Yuanbao, Tencent Cloud, and myapp. I received the Ph.D. degree from <a href="https://www.imperial.ac.uk/" target="_blank">Imperial College London</a>, UK, 2016, under the supervision of <a href="https://sites.google.com/view/tkkim/" target="_blank">Prof. Tae-Kyun Kim</a>, and working closely with <a href="https://bjornstenger.github.io/" target="_blank">Dr. Bjorn Stenger</a>, M.E. degree from <a href="http://english.ia.cas.cn/" target="_blank">Institute of Automation</a>, Chinese Academy of Sciences, China, 2012, under the supervision of <a href="https://scholar.google.com/citations?user=Wl4tl4QAAAAJ&amp;hl=en" target="_blank">Prof. Weiming Hu</a>, and B.E. degree from <a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology" target="_blank">Huazhong University of Science and Technology</a>, China, 2009.</p>

<p style="text-align:justify; text-justify:inter-ideograph;"> I have published over 90 peer-reviewed papers in top-tier conferences and journals, like ICML, NeurIPS, CVPR, ICCV, ECCV, SIGGRAPH, AAAI, ACL, ACMMM, ICLR, TPAMI, AI, IJCV. My work is selected into the CVPR 2019 Best Paper Finalist and I was awarded the 2022 ACM China Rising Star Award (Guangzhou Chapter). I have served as Associate Editor for IEEE Transactions on Image Processing, Neurocomputing, IET Computer Vision, Guest Editor for CVIU, Area Chair for NeurIPS 2025, ACM MM 2025, ICML 2025, IJCAI 2025, IJCNN 2025, BMVC 2024, and Senior Program Committee member for AAAI and IJCAI, regular reviewer for top conferences and journals like TPAMI, IJCV, CVPR, ICML, ICCV. I have been elected among Top 2% Scientists worldwide (2023 &amp; 2024) by Stanford/Elsevier. [<a href="assets/WenhanLuo_CV.pdf" target="_blank">Curriculum Vitae</a>] [<a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=wenhan-luo-whluo" target="_blank">HKUST Profile</a>] </p>

<p style="text-align:justify; text-justify:inter-ideograph;"><font color="a82e2e">Multiple PHD opennings in efficient large model, AIGC (image and video generation), content restoration and enhancement are available. Please check the Join Us page for details.</font></p>

<h1 id="research">Research</h1>

<p style="text-align:justify; text-justify:inter-ideograph;"> I conduct research on creative AI. Specifically, my current research focuses on several topics, such as image/video generation and restoration/enhancement. My research is supported by the following sponsors/agencies. </p>

<p><img src="../assets/logos/Samsung_Logo.png" align="center" width="200" hspace="20" />
<img src="../assets/logos/Tencent_Logo.png" align="center" width="200" hspace="20" />
<img src="../assets/logos/metax_logo.png" align="center" width="200" hspace="20" />
<img src="../assets/logos/Huawei_logo.png" align="center" width="100" hspace="20" />
<img src="../assets/logos/NSFC_logo.png" align="center" width="200" hspace="20" />
<img src="../assets/logos/GD_logo.png" align="center" width="200" hspace="20" />
<img src="../assets/logos/SZ_logo.jpeg" align="center" width="200" hspace="20" /></p>

<h1 id="updates">Updates</h1>
<ul>
  <li>2025/05:   Papers accepted by TVCG and ACL 2025.</li>
  <li>2025/05:   Serve as Area Chair for BMVC 2025.</li>
  <li>2025/04:   Two papers are accepted by SIGGRAPH 2025.</li>
  <li>2025/04:   Serve as COI Coordinator for SIGGRAPH Asia 2025.</li>
  <li>2025/04:   I will serve as Area Chair for NeurIPS 2025.</li>
  <li>2025/03:   Content restoration works to appear in TPAMI and PR.</li>
  <li>2025/03:   I serve as Workshop Chair for <a href="http://iccvm.org/2025/">CVM 2025</a>.</li>
  <li>2025/03:   I will serve as Area Chair for ACM Multimedia 2025.</li>
  <li>2025/02:   Three papers accepted by CVPR 2025.</li>
  <li>2025/01:   I will serve as Associate Editor for IEEE Transactions on Image Processing.</li>
  <li>2025/01:   Two papers accepted by ICLR 2025.</li>
  <li>2025/01:   Uni-MoE accepted by TPAMI.</li>
  <li>2024/12:   Invited to serve as Area Chair for IJCAI 2025.</li>
</ul>

<p><a onclick="toggleList()" id="more">Show more</a></p>
<div id="hiddenList" style="display:none;">
  
  
<ul>
  <li>2024/12:   Invited to serve as Area Chair for ICML 2025.</li>
  <li><del>2024/11:   Invited to serve as Senior Program Committe member (SPC) for IJCAI 2025.</del></li>
  <li>2024/10:   Awarded the CCF-Tencent Rhino-Bird Faculty Fund Excellence Award, see the <a href="https://mp.weixin.qq.com/s/90ajLg_GxG5dsap7s4qasw">news</a>.</li>
  <li>2024/09:   Two Papers accepted by NeurIPS 2024.</li>
  <li>2024/09:   Elected among Stanford/Elsevier Top 2% Scientists List 2024.</li>
  <li>2024/09:   Paper to appear in TKDE.</li>
  <li>2024/09:   Paper to appear in IJCV.</li>
  <li>2024/08:   Paper to appear in TCSVT.</li>
  <li>2024/07:   Paper accepted by ACM MM 2024.</li>
  <li>2024/07:   Four papers accepted by ECCV 2024.</li>
  <li>2024/06:   Invited to serve as Senior Program Committe member (SPC) for AAAI 2025.</li>
  <li>2024/05:   Invited to serve as Area Chair for BMVC 2024.</li>
  <li>2024/05:   Paper accepted by ICML 2024.</li>
  <li>2024/04:   Our team win two championship awards in the two tracks of <a href="https://cvlai.net/ntire/2024/">NTIRE 2024</a> challenges (Bracketing Image Restoration and Enhancement Challenge - Track 1 &amp; 2) in conjunction with CVPR2024, see the <a href="https://cvlai.net/ntire/2024/NTIRE2024awards_certificates.pdf">award</a>.</li>
  <li>2024/04:   Paper to appear in IEEE TGRS.</li>
  <li>2024/03:   Together with Samsung, we win the 2rd place in the competition of Few-shot RAW Image Denoising @ <a href="https://mipi-challenge.org/MIPI2024/">MIPI2024</a> in conjunction with CVPR2024, see the <a href="https://mipi-challenge.org/MIPI2024/award_certificates_2024.pdf">award</a>.</li>
  <li>2024/03:   Paper to appear in TCSVT.</li>
  <li>2024/03:   Paper to appear in IJCV.</li>
  <li>2024/02:   Two papers accepted by CVPR 2024.</li>
  <li>2024/02:   Paper accepted by COLING 2024 (multi-modal in-context learning).</li>
  <li>2024/02:   I will be joining the Hong Kong University of Science and Technology (HKUST) as Associate Professor in 2024 spring/summer.</li>
  <li>2024/01:   Paper accepted by ICLR 2024.</li>
  <li>2023/12:   Paper to appear in TNNLS (blind face restoration).</li>
  <li>2023/12:   Invited to serve as Senior Program Committe member (SPC) for IJCAI 2024.</li>
  <li>2023/11:   Paper to appear in TCSVT (under-display camera face image restoration).</li>
  <li>2023/10:   Elected among Top 2% Scientists worldwide 2023 by Stanford University.</li>
  <li>2023/09:   Paper accepted by NeurIPS 2023 (punctuation-level attacks fooling text models).</li>
  <li>2023/09:   Paper to appear in TCSVT (image deblurring benchmark).</li>
  <li>2023/09:   Paper to appear in Pattern Recognition (image dehazing).</li>
  <li>2023/07:   I was granted the CCF-Tencent Rhino-Bird Faculty Research Fund, see the <a href="https://www.ccf.org.cn/Collaboration/Enterprise_Fund/News/tx/2023-07-31/794523.shtml">news</a>.</li>
  <li>2023/07:   Paper to apppear in TCSVT (all-in-one weather-degraded image restoration).</li>
  <li>2023/07:  Four papers accepted by ICCV 2023.</li>
  <li>2023/07:  Invited to serve as a member of Senior Program Committe (SPC) for AAAI 2024.</li>
  <li>2023/06:  Paper accepted by IROS 2023 (hand interaction tracking).</li>
  <li>2023/02:  Paper accepted by CVPR 2023 (reflection removal against adversarial attacks).</li>
  <li>2023/02:  Elevation to IEEE Senior Member.</li>
  <li>2023/01:  Paper accepted by TNNLS (presentation attack detection).</li>
  <li>2022/12:  Invited to serve as a member of Senior Program Committe (SPC) for IJCAI 2023.</li>
  <li>2022/11:  Paper accepted by AAAI2023 (low-light image enhancement).</li>
  <li>2022/11:  TMM paper accepted (multi-modal retrieval).</li>
  <li>2022/10:  TMM paper accepted (image dehazing).</li>
  <li>2022/09:  Awarded the 2022 ACM China Rising Star Award (Guangzhou Chapter), see the <a href="https://www.scholat.com/vpost.html?pid=199904">news</a>.</li>
  <li>2022/08:  Paper accepted by WACV2023 (few-shot object counting).</li>
  <li>2022/07:  I joined Sun Yat-sen University as an associate professor.</li>
  <li>2022/06:  Paper accepted by ACM MM 2022 (multi-object tracking).</li>
  <li>2022/05:  TPAMI paper accepted (face hallucination).</li>
  <li>2022/05:  Paper accepted by IJCV (image deblurring survey).</li>
  <li>2022/04:  Paper accepted by IJCV (image deraining).</li>
  <li>2022/03:  Paper accepted by CVPR2022 (aesthetic text logo synthesis).</li>
  <li>2022/01:  TPAMI paper accepted (video deraining).</li>
  <li>2021/09:  TIP paper accepted (image deraining).</li>
  <li>2021/08:  Invited to serve as a Senior PC member for AAAI 2022.</li>
  <li>2021/08:  TIP paper accepted (image desnow).</li>
  <li>2021/07:  Paper accepted by ICCV2021 (image SR benchmarking).</li>
  <li>2021/07:  One paper (image deblur &amp; SR) to appear in IEEE Transactions on Image Processing.</li>
  <li>2021/06:  TMM paper accepted (action recognition).</li>
  <li>2021/05:  Our work of active visual tracking is accepted by ICML2021.</li>
  <li>2021/05:  One paper of image dehazing to appear in IEEE Transactions on Image Processing.</li>
  <li>2021/04:  Our work of human image synthesis is accepted to appear in TPAMI.</li>
  <li>2021/03:  One paper to appear in IEEE Transactions on Geoscience and Remote Sensing.</li>
  <li>2021/02:  One paper to appear in IEEE Transactions on Multimedia.</li>
  <li>2020/12:  The paper “Multiple Object Tracking: A Literature Review” is accepted by Artificial Intelligence.</li>
  <li>2020/12:  Invited to serve as a Senior PC member for IJCAI 2021.</li>
  <li>2020/11:  One paper of pedestrian detection to appear in IEEE Transactions on Image Processing.</li>
  <li>2020/09:  An invited talk is given in SUSTech, hosted by <a href="https://eee.sustc.edu.cn/?view=%E5%94%90%E6%99%93%E9%A2%96&amp;jsid=18&amp;lang=en">Prof. Xiaoying Tang</a>.</li>
  <li>2020/09:  One paper of optical flow estimation to appear in IEEE Transactions on Image Processing.</li>
  <li>2020/07:  One paper to appear in ACM MM 2020 (Oral).</li>
  <li>2020/07:  One paper to appear in ECCV2020.</li>
  <li>2020/06:  One paper of multiple object tracking to appear in Pattern Recognition.</li>
  <li>2020/05:  We are organizing a special issue of action recognition and detection on CVIU. Submission deadline is Sep 15th. See the <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding/call-for-papers/modeling-methodology-and-applications-of-action-recognition">CFP</a> if interested</li>
  <li>2020/02:  Two papers (one oral + one poster) to appear in CVPR2020.</li>
  <li>2019/10:  One paper to appear in TPAMI, entitled “AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking”.</li>
  <li>2019/09:  Code and dataset of our ICCV2019 paper for motion imitation, appearance transfer and novel view synthesis are released. Check the project page <a href="https://svip-lab.github.io/project/impersonator">here</a>.</li>
  <li>2019/09:  One paper to appear in IJCV.</li>
  <li>2019/07:  One paper to appear in ICCV2019.</li>
  <li>2019/07:  The code of our ACL2019 paper is released. Check it <a href="https://github.com/zfchenUnique/WSSTG">here</a>.</li>
  <li>2019/07:  The code of our ICLR2019 paper is released. Check it <a href="https://github.com/zfw1226/active_tracking_rl">here</a>.</li>
  <li>2019/05:  Join Amazon in California as a research scientist.</li>
  <li>2019/06:  Our CVPR paper entitled “Learning to Compose Dynamic Tree Structures for Visual Context” is selected as one of the best paper finalists (50 out of the 1294 accepted papers in CVPR2019).</li>
  <li>2019/05:  One paper of video grounding to appear in ACL2019 as a long paper, and oral presentation.</li>
  <li>2019/05:  Serve as program committee member of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drones 2019: A Challenge</a> in conjunction with ICCV2019.</li>
  <li>2019/03:  Four papers (2 orals + 2 posters) to appear in CVPR2019.</li>
  <li>2019/02:  Serve as program committee member of the <a href="https://motchallenge.net/workshops/bmtt2019/index.html">4th BMTT MOT Challenge Workshop</a> and the <a href="https://reid-mct.github.io/2019/">2nd Workshop and Challenge on Target Re-identification and Multi-Target Multi-Camera Tracking</a> in conjunction with CVPR2019.</li>
  <li>2019/02:  The code of our CVPR 2018 paper is released. Check it here.</li>
  <li>2019/01:  Our work of “<a href="https://arxiv.org/abs/1808.03405">End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning</a>” is accepted by TPAMI.</li>
  <li>2018/12:  One paper to appear in ICLR2019. Congratulations to Fangwei Zhong.</li>
  <li>2018/11:  One paper to appear in <a href="https://sites.google.com/view/deep-rl-workshop-nips-2018/home">NIPS2018 workshop on Deep Reinforcement Learning</a>.</li>
  <li>2018/11:  One paper to appear in AAAI2019. Congratulations to Kaihao Zhang.</li>
  <li>2018/10:  Welcome Tianrui Liu (Imperial College London) on board as a research intern.</li>
  <li>2018/09:  The code of our ICML2018 paper is released. Check it <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018">here</a>.</li>
  <li>2018/09:  The code of our ECCV2018 paper “Bi-Real Net” is released. Check it <a href="https://github.com/liuzechun/Bi-Real-net">here</a>.</li>
  <li>2018/08:  Our work of video deblur is accepted by IEEE Transactions on Image Processing. Congratulations to Kaihao Zhang.</li>
  <li>2018/08:  One paper of multiple object tracking is accepted by IEEE Transactions on Image Processing.</li>
  <li>2018/07:  The dataset for our CVPR2018 paper, Sky Scene is released. See our <a href="https://sites.google.com/site/whluoimperial/mdgan">project page</a> for details.</li>
  <li>2018/07:  One paper to appear in ECCV2018. Congratulations to Zechun Liu.</li>
  <li>2018/05:  Our work of End-to-end Active Object Tracking via Reinforcement Learning is accepted by ICML2018. The camera-ready version will come soon.</li>
  <li>2018/05:  Serve as a member of the advisory committee of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drone: A Challenge</a> (VisDrone2018, for short) in conjunction with ECCV2018.</li>
  <li>2018/04:  Welcome Jia Wan (NWPU) on board as an intern.</li>
  <li>2018/02:  One paper to appear in CVPR2018. Congratulations to Wei Xiong.</li>
  <li>2018/01:  Welcome our intern Zechun Liu (HKUST) on board.</li>
  <li>2017/10:  Welcome Yiming Chen (Imperial College London) on board as an intern in Tencent AI Lab.</li>
  <li>2017/08:  Welcome Kaihao Zhang on board as intern in Tencent AI Lab. Kaihao is from Australian National University and will work close with me for about six months.</li>
  <li>2017/06:  Welcome Weiyue Su and Wei Xiong on board as intern in Tencent AI Lab. Weiyue is from South China University of Technology. Wei is from Wuhan University.</li>
  <li>2017/05:  Serve as a program committee member of the First <a href="https://motchallenge.net/workshops/bmtt-pets2017/index.html">Joint BMPP-PETS Workshop on Tracking and Surveillance</a> in conjunction with CVPR2017.</li>
  <li>2017/04:  One paper to appear in CVPR2017.</li>
  <li>2016/07:  Join Tencent AI Lab as a research scientist.</li>
  <li>2016/06:  Pass viva exam and obtained the Ph.D. degree, examined by <a href="https://www.eecs.qmul.ac.uk/~txiang/">Tao (Tony) Xiang</a> from Queen Mary University of London.</li>
  <li>2016/05:  Serve as a program committee member of the workshop <a href="https://motchallenge.net/workshops/bmtt2016/">Benchmarking Multi-Target Tracking: MOTChallenge</a> in conjunction with ECCV2016.</li>
  <li>2015/02:  Start internship in Microsoft Research Asia with <a href="http://www.davidwipf.com/">Dr. David Wipf</a> (from Feb 2015 to June 2015).</li>
</ul>


</div>
<p><a onclick="toggleList()" id="less" style="display:none;">Show less</a>
<script>
function toggleList() {
	var list = document.getElementById('hiddenList');
	list.style.display = list.style.display === 'none' ? 'block' : 'none';
	var button = document.getElementById('more');
	button.style.display = button.style.display === 'none' ? 'block' : 'none';
	var buttom_less = document.getElementById('less');
	buttom_less.style.display = buttom_less.style.display === 'none' ? 'block' : 'none';
}
</script></p>

<h1 id="recent-work-more">Recent Work <a href="assets/publication.html"><strong>[More]</strong></a></h1>
<table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:0px;">
    <tbody>
     
    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/unic_demo.gif" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>UNIC: Unified In-Context Video Editing</papertitle>,
                  <br />Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, Wenhan Luo,<br />
                  <i>arXiv:2506.04216.</i>
                  <br />
                  [<a href="https://arxiv.org/abs/2506.04216" target="_blank">arXiv</a>]
                  [<a href="https://zixuan-ye.github.io/UNIC/" target="_blank">Project Page</a>]
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/multitalk.gif" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</papertitle>,
                  <br />Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, Wenhan Luo,<br />
                  <i>arXiv:2505.22647.</i>
                  <br />
                  [<a href="https://arxiv.org/abs/2505.22647" target="_blank">arXiv</a>]
                  [<a href="https://meigen-ai.github.io/multi-talk/" target="_blank">Project Page</a>]
                  [<a href="https://github.com/MeiGen-AI/MultiTalk/" target="_blank">Code</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/MeiGen-AI/MultiTalk?style=social" />
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/materialmvp.gif" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion</papertitle>,
                  <br />Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen, Liu Yuhong, Jie Jiang, Chunchao Guo, Wenhan Luo,<br />
                  <i>arXiv:2503.10289, 2025.</i>
                  <br />
                  [<a href="https://arxiv.org/abs/2503.10289" target="_blank">arXiv</a>]
                  [<a href="https://zebinhe.github.io/MaterialMVP/" target="_blank">Project Page</a>]
                  [<a href="https://github.com/ZebinHe/MaterialMVP" target="_blank">Code</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ZebinHe/MaterialMVP?style=social" />
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/dam-vsr-siggraph.jpg" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</papertitle>,
                  <br />Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo,<br />
                  <i>ACM SIGGRAPH, 2025.</i>
                  <br />
                  [<a href="" target="_blank">PDF</a>]
                  [<a href="https://kongzhecn.github.io/projects/dam-vsr/" target="_blank">Project Page</a>]
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/mbtaylorv2.jpg" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by Taylor Formula for Image Restoration</papertitle>,
                  <br />Zhi Jin, Yuwei Qiu, Kaihao Zhang, Hongdong Li, Wenhan Luo,<br />
                  <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), to appear.</i>
                  <br />
                  [<a href="https://arxiv.org/abs/2501.04486" target="_blank">arXiv</a>]
                  [<a href="https://github.com/FVL2020/MB-TaylorFormerV2" target="_blank">Code</a>]
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/stylemaster.gif" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>StyleMaster: Stylize Your Video with Artistic Generation and Translation</papertitle>,
                  <br />Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, Wenhan Luo,<br />
                  <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025.</i>
                  <br />
                  [<a href="https://arxiv.org/pdf/2412.07744" target="_blank">arXiv</a>]
                  [<a href="https://github.com/KwaiVGI/StyleMaster" target="_blank">Github</a>]
                  [<a href="https://zixuan-ye.github.io/stylemaster/" target="_blank">Project Page</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/KwaiVGI/StyleMaster?style=social" />
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src="../assets/multimedia/pose_animation.gif" style="max-width: 100%; width: 100%;" />
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling</papertitle>,
        <br />Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo,<br />
        <i>International Conference on Learning Representations (ICLR), 2025.</i>
        <br />
        [<a href="https://openreview.net/forum?id=aqlzXgXwWa" target="_blank">PDF</a>]
        [<a href="https://multi-animation.github.io/" target="_blank">Project Page</a>]
        [<a href="https://cloud.tencent.com/product/vclm" target="_blank">API in Tencent Cloud</a>]
      </td>
    </tr>

  </tbody>
</table>

<h1 id="experience">Experience</h1>

<p>I have studied/interned/worked in the following affiliations.</p>

<p><a href="https://hkust.edu.hk/"><img src="../assets/logos/HKUST.png" align="center" width="40" hspace="5" /></a>
<a href="https://en.wikipedia.org/wiki/Sun_Yat-sen_University"><img src="../assets/logos/SYSU.png" align="center" width="60" hspace="5" /></a>
<a href="https://en.wikipedia.org/wiki/Tencent"><img src="../assets/logos/Tencent.png" align="center" width="80" hspace="5" /></a>
<a href="https://en.wikipedia.org/wiki/Amazon_(company)"><img src="../assets/logos/Amazon.png" align="center" width="70" hspace="5" /></a>
<a href="https://research.ibm.com/"><img src="../assets/logos/IBM.png" align="center" width="80" hspace="5" /></a>
<a href="https://www.imperial.ac.uk/"><img src="../assets/logos/Imperial.png" align="center" width="50" hspace="5" /></a>
<a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/"><img src="../assets/logos/MSRA.png" align="center" width="100" hspace="5" /></a>
<a href="http://english.ia.cas.cn/"><img src="../assets/logos/CASIA.jpg" align="center" width="60" hspace="5" /></a>
<a href="https://en.wikipedia.org/wiki/Huazhong_University_of_Science_and_Technology"><img src="../assets/logos/HUST.png" align="center" width="80" hspace="5" /></a>
<img src="../assets/logos/Middleschool.jpg" align="center" width="60" hspace="5" /></p>

<p class="footer">
                  <span id="busuanzi_container_site_pv"># Views: <span id="busuanzi_value_site_pv"></span> (Since Dec 11, 2024)</span>
                </p>

<script>
function pubtoggleList() {
	var list = document.getElementById('pubhiddenList');
	list.style.display = list.style.display === 'none' ? 'block' : 'none';
	var button = document.getElementById('pubmore');
	button.style.display = button.style.display === 'none' ? 'block' : 'none';
	var buttom_less = document.getElementById('publess');
	buttom_less.style.display = buttom_less.style.display === 'none' ? 'block' : 'none';
}
</script>


          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/whluo/whluo.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
