<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Wenhan Luo - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Wenhan Luo">
<meta property="og:title" content="Wenhan Luo">


  <link rel="canonical" href="http://localhost:4000/">
  <meta property="og:url" content="http://localhost:4000/">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About</a></li>
          
            <li class="masthead__menu-item"><a href="/#research">Research</a></li>
          
            <li class="masthead__menu-item"><a href="/#updates">Updates</a></li>
          
            <li class="masthead__menu-item"><a href="/#publications-full-list">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="assets/team.html">Team</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/wenhan_square.png" class="author__avatar" alt="Wenhan Luo">
  </div>

  <div class="author__content">
    <h3 class="author__name">Wenhan Luo</h3>
    <p class="author__bio">Associate Professor, HKUST</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;"></div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Clear Water Bay, Hong Kong</li>
      
      
      
      
        <li><a href="mailto:whluo@ust.hk"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=g20Q12MAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:whluo.china@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com.hk/citations?user=g20Q12MAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<script type="text/javascript">
	$(document).ready(function() {
		$('a[href^="http"]').each(function() {
			$(this).attr('target', '_blank');
		});
	});
</script>

<p style="text-align:justify; text-justify:inter-ideograph;">
<font color="a82e2e">
I am looking for 3~4 PhD students (starting at 2024 autumn or 2025 spring), 1 post-doc (starting at any time), multiple interns and RA (starting at any time, can be onsite or remote) to work with me at HKUST. Please <a href="mailto:whluo.china@gmail.com" target="_blank">Email</a> me (whluo.china AT gmail.com) with your CV and indicate the position you are interested. Applicants seeking for PhD opportunities should have a strong track record in CVPR/ICCV/ECCV, NIPS/ICML/ICLR, or PAMI/IJCV. Candidates who are able to carry highest quality research independently are pursued.</font></p>

<p style="text-align:justify; text-justify:inter-ideograph;">I will be joining the Hong Kong University of Science and Technology (HKUST) as Associate Professor in 2024 spring/summer.</p>

<p style="text-align:justify; text-justify:inter-ideograph;">I conduct research on creative AI. Previously, I worked as an Associate Professor at Sun Yat-sen University, and an applied research scientist at Tencent, solving real-world problems using computer vision and machine learning techniques. Prior to Tencent, I worked for Amazon (A9) in Palo Alto, California, where I developed deep models for better visual search experience. Before that, I worked as a research scientist in Tencent AI Lab. The techniques I have developed/involved have been shipped to several products in Tencent such as WeChat, QQ, Tencent Video, and myapp. I received the Ph.D. degree from Imperial College London, UK, 2016, M.E. degree from Institute of Automation, Chinese Academy of Sciences, China, 2012 and B.E. degree from Huazhong University of Science and Technology, China, 2009. I have published over 80 peer-reviewed papers, most of which are published in top-tier conferences and journals, like ICML, NeurIPS, CVPR, ICCV, ECCV, AAAI, ACL, ACMMM, ICLR, TPAMI, AI, IJCV, TIP. I received the CVPR 2019 Best Paper Nominee and was awarded the 2022 ACM China Rising Star Award (Guangzhou Chapter). I have been elected among Top 2% Scientists worldwide 2023 by Stanford university. [<a href="assets/WenhanLuo_CV.pdf" target="_blank">CV</a>] </p>

<h1 id="research">Research</h1>

<p>I am interested in several topics in computer vision and machine learning. Specifically, my current research focuses on creative AI, such as image/video synthesis and enhancement.</p>

<h1 id="updates">Updates</h1>
<ul>
  <li>2024/04:   Our team win two championship awards in the two tracks of <a href="https://cvlai.net/ntire/2024/">NTIRE 2024</a> challenges (Bracketing Image Restoration and Enhancement Challenge - Track 1 &amp; 2) in conjunction with CVPR2024.</li>
  <li>2024/04:   Paper to appear in IEEE TGRS.</li>
  <li>2024/03:   Together with Samsung, we win the 2rd place in the competition of Few-shot RAW Image Denoising @ <a href="https://mipi-challenge.org/MIPI2024/">MIPI2024</a> in conjunction with CVPR2024, see the <a href="https://mipi-challenge.org/MIPI2024/award_certificates_2024.pdf">award</a>.</li>
  <li>2024/03:   Paper to appear in TCSVT.</li>
  <li>2024/03:   Paper to appear in IJCV.</li>
  <li>2024/02:   Two papers accepted by CVPR 2024.</li>
  <li>2024/02:   Paper accepted by COLING 2024 (multi-modal in-context learning).</li>
  <li>2024/02:   I will be joining the Hong Kong University of Science and Technology (HKUST) as Associate Professor in 2024 spring/summer.</li>
  <li>2024/01:   Paper accepted by ICLR 2024.</li>
  <li>2023/12:   Paper to appear in TNNLS (blind face restoration).</li>
  <li>2023/12:   Invited to serve as Senior Program Committe member (SPC) for IJCAI 2024.</li>
  <li>2023/11:   Paper to appear in TCSVT (under-display camera face image restoration).</li>
  <li>2023/10:   Elected among Top 2% Scientists worldwide 2023 by Stanford University.</li>
</ul>

<p><a onclick="toggleList()" id="more">Show more</a></p>
<div id="hiddenList" style="display:none;">
  
  
<ul>
  <li>2023/09:   Paper accepted by NeurIPS 2023 (punctuation-level attacks fooling text models).</li>
  <li>2023/09:   Paper to appear in TCSVT (image deblurring benchmark).</li>
  <li>2023/09:  Paper to appear in Pattern Recognition (image dehazing).</li>
  <li>2023/07:  I was granted the CCF-Tencent Rhino-Bird Young Faculty Open Research Fund, see the <a href="https://www.ccf.org.cn/Collaboration/Enterprise_Fund/News/tx/2023-07-31/794523.shtml">news</a>.</li>
  <li>2023/07:  Paper to apppear in TCSVT (all-in-one weather-degraded image restoration).</li>
  <li>2023/07:  Four papers accepted by ICCV 2023.</li>
  <li>2023/07:  Invited to serve as a member of Senior Program Committe (SPC) for AAAI 2024.</li>
  <li>2023/06:  Paper accepted by IROS 2023 (hand interaction tracking).</li>
  <li>2023/02:  Paper accepted by CVPR 2023 (reflection removal against adversarial attacks).</li>
  <li>2023/02:  Elevation to IEEE Senior Member.</li>
  <li>2023/01:  Paper accepted by TNNLS (presentation attack detection).</li>
  <li>2022/12:  Invited to serve as a member of Senior Program Committe (SPC) for IJCAI 2023.</li>
  <li>2022/11:  Paper accepted by AAAI2023 (low-light image enhancement).</li>
  <li>2022/11:  TMM paper accepted (multi-modal retrieval).</li>
  <li>2022/10:  TMM paper accepted (image dehazing).</li>
  <li>2022/09:  Awarded the 2022 ACM China Rising Star Award (Guangzhou Chapter), see the <a href="https://www.scholat.com/vpost.html?pid=199904">news</a>.</li>
  <li>2022/08:  Paper accepted by WACV2023 (few-shot object counting).</li>
  <li>2022/07:  I joined Sun Yat-sen University as an associate professor.</li>
  <li>2022/06:  Paper accepted by ACM MM 2022 (multi-object tracking).</li>
  <li>2022/06:  Paper accepted by KBS (generative model for facial expression recognition).</li>
  <li>2022/05:  TPAMI paper accepted (face hallucination).</li>
  <li>2022/05:  Paper accepted by IJCV (image deblurring survey).</li>
  <li>2022/04:  Paper accepted by IJCV (image deraining).</li>
  <li>2022/03:  Paper accepted by CVPR2022 (aesthetic text logo synthesis).</li>
  <li>2022/01:  TPAMI paper accepted (video deraining).</li>
  <li>2021/09:  TIP paper accepted (image deraining).</li>
  <li>2021/08:  Invited to serve as a Senior PC member for AAAI 2022.</li>
  <li>2021/08:  TIP paper accepted (image desnow).</li>
  <li>2021/07:  Paper accepted by ICCV2021 (image SR benchmarking).</li>
  <li>2021/07:  One paper (image deblur &amp; SR) to appear in IEEE Transactions on Image Processing.</li>
  <li>2021/06:  TMM paper accepted (action recognition).</li>
  <li>2021/05:  Our work of active visual tracking is accepted by ICML2021.</li>
  <li>2021/05:  One paper of image dehazing to appear in IEEE Transactions on Image Processing.</li>
  <li>2021/04:  Our work of human image synthesis is accepted to appear in TPAMI.</li>
  <li>2021/03:  One paper to appear in IEEE Transactions on Geoscience and Remote Sensing.</li>
  <li>2021/02:  One paper to appear in IEEE Transactions on Multimedia.</li>
  <li>2020/12:  The paper “Multiple Object Tracking: A Literature Review” is accepted by Artificial Intelligence.</li>
  <li>2020/12:  Invited to serve as a Senior PC member for IJCAI 2021.</li>
  <li>2020/11:  One paper of pedestrian detection to appear in IEEE Transactions on Image Processing.</li>
  <li>2020/09:  An invited talk is given in SUSTech, hosted by <a href="https://eee.sustc.edu.cn/?view=%E5%94%90%E6%99%93%E9%A2%96&amp;jsid=18&amp;lang=en">Prof. Xiaoying Tang</a>.</li>
  <li>2020/09:  One paper of optical flow estimation to appear in IEEE Transactions on Image Processing.</li>
  <li>2020/07:  One paper to appear in ACM MM 2020 (Oral).</li>
  <li>2020/07:  One paper to appear in ECCV2020.</li>
  <li>2020/06:  One paper of multiple object tracking to appear in Pattern Recognition.</li>
  <li>2020/05:  We are organizing a special issue of action recognition and detection on CVIU. Submission deadline is Sep 15th. See the <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding/call-for-papers/modeling-methodology-and-applications-of-action-recognition">CFP</a> if interested</li>
  <li>2020/02:  Two papers (one oral + one poster) to appear in CVPR2020.</li>
  <li>2019/10:  One paper to appear in TPAMI, entitled “AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking”.</li>
  <li>2019/09:  Code and dataset of our ICCV2019 paper for motion imitation, appearance transfer and novel view synthesis are released. Check the project page <a href="https://svip-lab.github.io/project/impersonator">here</a>.</li>
  <li>2019/09:  One paper to appear in IJCV.</li>
  <li>2019/07:  One paper to appear in ICCV2019.</li>
  <li>2019/07:  The code of our ACL2019 paper is released. Check it <a href="https://github.com/zfchenUnique/WSSTG">here</a>.</li>
  <li>2019/07:  The code of our ICLR2019 paper is released. Check it <a href="https://github.com/zfw1226/active_tracking_rl">here</a>.</li>
  <li>2019/05:  Join Amazon in California as a research scientist.</li>
  <li>2019/06: Our CVPR paper entitled “Learning to Compose Dynamic Tree Structures for Visual Context” is selected as one of the best paper finalists (50 out of the 1294 accepted papers in CVPR2019).</li>
  <li>2019/05:  One paper of video grounding to appear in ACL2019 as a long paper, and oral presentation.</li>
  <li>2019/05:  Serve as program committee member of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drones 2019: A Challenge</a> in conjunction with ICCV2019.</li>
  <li>2019/03:  Four papers (2 orals + 2 posters) to appear in CVPR2019.</li>
  <li>2019/02:  Serve as program committee member of the <a href="https://motchallenge.net/workshops/bmtt2019/index.html">4th BMTT MOT Challenge Workshop</a> and the <a href="https://reid-mct.github.io/2019/">2nd Workshop and Challenge on Target Re-identification and Multi-Target Multi-Camera Tracking</a> in conjunction with CVPR2019.</li>
  <li>2019/02:  The code of our CVPR 2018 paper is released. Check it here.</li>
  <li>2019/01:  Our work of “<a href="https://arxiv.org/abs/1808.03405">End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning</a>” is accepted by TPAMI.</li>
  <li>2018/12:  One paper to appear in ICLR2019. Congratulations to Fangwei Zhong.</li>
  <li>2018/11:  One paper to appear in <a href="https://sites.google.com/view/deep-rl-workshop-nips-2018/home">NIPS2018 workshop on Deep Reinforcement Learning</a>.</li>
  <li>2018/11:  One paper to appear in AAAI2019. Congratulations to Kaihao Zhang.</li>
  <li>2018/10:  Welcome Tianrui Liu (Imperial College London) on board as a research intern.</li>
  <li>2018/09:  The code of our ICML2018 paper is released. Check it <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018">here</a>.</li>
  <li>2018/09:  The code of our ECCV2018 paper “Bi-Real Net” is released. Check it <a href="https://github.com/liuzechun/Bi-Real-net">here</a>.</li>
  <li>2018/08:  Our work of video deblur is accepted by IEEE Transactions on Image Processing. Congratulations to Kaihao Zhang.</li>
  <li>2018/08:  One paper of multiple object tracking is accepted by IEEE Transactions on Image Processing.</li>
  <li>2018/07:  The dataset for our CVPR2018 paper, Sky Scene is released. See our <a href="https://sites.google.com/site/whluoimperial/mdgan">project page</a> for details.</li>
  <li>2018/07:  One paper to appear in ECCV2018. Congratulations to Zechun Liu.</li>
  <li>2018/05:  Our work of End-to-end Active Object Tracking via Reinforcement Learning is accepted by ICML2018. The camera-ready version will come soon.</li>
  <li>2018/05:  Serve as a member of the advisory committee of the workshop of <a href="http://aiskyeye.com/">Vision Meets Drone: A Challenge</a> (VisDrone2018, for short) in conjunction with ECCV2018.</li>
  <li>2018/04:  Welcome Jia Wan (NWPU) on board as an intern.</li>
  <li>2018/02:  One paper to appear in CVPR2018. Congratulations to Wei Xiong.</li>
  <li>2018/01:  Welcome our intern Zechun Liu (HKUST) on board.</li>
  <li>2017/10:  Welcome Yiming Chen (Imperial College London) on board as an intern in Tencent AI Lab.</li>
  <li>2017/08:  Welcome Kaihao Zhang on board as intern in Tencent AI Lab. Kaihao is from Australian National University and will work close with me for about six months.</li>
  <li>2017/06:  Welcome Weiyue Su and Wei Xiong on board as intern in Tencent AI Lab. Weiyue is from South China University of Technology. Wei is from Wuhan University.</li>
  <li>2017/05:  Serve as a program committee member of the First <a href="https://motchallenge.net/workshops/bmtt-pets2017/index.html">Joint BMPP-PETS Workshop on Tracking and Surveillance</a> in conjunction with CVPR2017.</li>
  <li>2017/04:  One paper to appear in CVPR2017.</li>
  <li>2016/07:  Join Tencent AI Lab as a research scientist.</li>
  <li>2016/06:  Pass viva exam and obtained the Ph.D. degree, examined by <a href="https://www.eecs.qmul.ac.uk/~txiang/">Tao (Tony) Xiang</a> from Queen Mary University of London.</li>
  <li>2016/05:  Serve as a program committee member of the workshop <a href="https://motchallenge.net/workshops/bmtt2016/">Benchmarking Multi-Target Tracking: MOTChallenge</a> in conjunction with ECCV2016.</li>
  <li>2015/02:  Start internship in Microsoft Research Asia with <a href="http://www.davidwipf.com/">Dr. David Wipf</a> (from Feb 2015 to June 2015).</li>
</ul>


</div>
<p><a onclick="toggleList()" id="less" style="display:none;">Show less</a>
<script>
function toggleList() {
	var list = document.getElementById('hiddenList');
	list.style.display = list.style.display === 'none' ? 'block' : 'none';
	var button = document.getElementById('more');
	button.style.display = button.style.display === 'none' ? 'block' : 'none';
	var buttom_less = document.getElementById('less');
	buttom_less.style.display = buttom_less.style.display === 'none' ? 'block' : 'none';
}
</script></p>

<h1 id="publications-full-list">Publications <a href="assets/publication.html"><strong>[Full List]</strong></a></h1>
<p>(* indicates equal contribution, + indicates intern/student working with me, # indicates correspondence)</p>

<ul>
  <li>
    <p>GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions,</p>

    <p>Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo#, Bjorn Stenger, Tong Lu, Tae-Kyun Kim, Wei Liu, Hongdong Li,</p>

    <p>International Journal of Computer Vision (IJCV), to appear.</p>

    <p><a href="https://arxiv.org/abs/2305.17863"><strong>[PDF]</strong></a> <a href=""><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation,</p>

    <p>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024.</p>

    <p><a href="https://arxiv.org/abs/2311.17532"><strong>[PDF]</strong></a> <a href=""><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Context-Aware Integration of Language and Visual References for Natural Language Tracking,</p>

    <p>Yanyan Shao, Shuting He, Qi Ye, Yuchao Feng, Wenhan Luo, Jiming Chen,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024.</p>

    <p><a href="https://arxiv.org/abs/2403.19975"><strong>[PDF]</strong></a> <a href=""><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost,</p>

    <p>Yuan Gao, Weizhong Zhang, Wenhan Luo, Lin Ma, Jin-Gang Yu, Gui-Song Xia, Jiayi Ma,</p>

    <p>International Conference on Learning Representations (ICLR), 2024.</p>

    <p><a href="https://openreview.net/forum?id=cINwAhrgLf&amp;noteId=cINwAhrgLf"><strong>[PDF]</strong></a> <a href=""><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models,</p>

    <p>Wenqiang Wang, Chongyang Du, Tao Wang, Kaihao Zhang, Wenhan Luo#, Lin Ma, Wei Liu, Xiaochun Cao,</p>

    <p>Neural Information Processing Systems (NeurIPS), 2023.</p>

    <p><a href="https://openreview.net/pdf?id=ir6WWkFR80"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>FnF Attack: Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives,</p>

    <p>Tao Zhou, Qi Ye#, Wenhan Luo#, Kaihao Zhang, Zhiguo Shi, Jiming Chen,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</p>

    <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_FF_Attack_Adversarial_Attack_against_Multiple_Object_Trackers_by_Inducing_ICCV_2023_paper.pdf"><strong>[PDF]</strong></a> <a href="https://infzhou.github.io/FnFAttack/index.html"><strong>[Project]</strong></a> <a href="https://github.com/infZhou/FnF_Attack"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>PRIOR: Prototype Representation Joint Learning from Medical Images and Reports,</p>

    <p>Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</p>

    <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/QtacierP/PRIOR"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>MB-TaylorFormer: Mutil-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing,</p>

    <p>Yuwei Qiu, Kaihao Zhang, Chenxi Wang, Wenhan Luo, Hongdong Li, Zhi Jin,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</p>

    <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_MB-TaylorFormer_Multi-Branch_Efficient_Transformer_Expanded_by_Taylor_Formula_for_Image_ICCV_2023_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Homography Guided Temporal Fusion for Road Line and Marking Segmentation,</p>

    <p>Shan Wang, Chuong Nguyen, Jiawei Liu, Kaihao Zhang, Wenhan Luo, Yanhao Zhang, Sundaram Muthu, Fahira Afzal Maken, Hongdong Li,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</p>

    <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/ShanWang-Shan/HomoFusion"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Robust Single Image Reflection Removal Against Adversarial Attacks,</p>

    <p>Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo#, Zhaoxin Fan, Wenqi Ren, Jianfeng Lu,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2023.</p>

    <p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/ZhenboSong/RobustSIRR"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Benchmarking Ultra-High-Definition Low-Light Image Enhancement and A Transformer Method,</p>

    <p>Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo#, Bjorn Stenger, Tong Lu#,</p>

    <p>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), USA, 2023. (<font color="red">Oral</font>)</p>

    <p><a href="https://arxiv.org/abs/2212.11548"><strong>[PDF]</strong></a> <a href="https://github.com/TaoWangzj/LLFormer"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>APPTracker: Improving Tracking Multiple Objects in Low-Frame-Rate Videos,</p>

    <p>Tao Zhou, Wenhan Luo, Zhiguo Shi, Jiming Chen, Qi Ye,</p>

    <p>The 30th ACM International Conference on Multimedia (ACM MM), 2022.</p>

    <p><a href="https://infzhou.github.io/folder/Zhou_APPTracker_Improving_Tracking_Multiple_Objects_in_Low-Frame-Rate_Videos_MM_2022.pdf"><strong>[PDF]</strong></a> <a href="https://infzhou.github.io/appTracker/index.html"><strong>[Project Page]</strong></a></p>
  </li>
  <li>
    <p>EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset,</p>

    <p>Kaihao Zhang, Dongxu Li, Wenhan Luo, Jingyu Liu, Jiankang Deng, Wei Liu, Stefanos Zafeiriou,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 45, pp. 3968-3978, 2023.</p>

    <p><a href="https://arxiv.org/abs/2110.05031"><strong>[PDF]</strong></a> <a href="https://github.com/HDCVLab/EDFace-Celeb-1M"><strong>[Github]</strong></a> <a href="https://zhangkaihao.github.io/projects/EDface/"><strong>[Project Page]</strong></a></p>
  </li>
  <li>
    <p>Deep Image Deblurring: A Survey,</p>

    <p>Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bjorn Stenger, Ming-Hsuan Yang, Hongdong Li,</p>

    <p>International Journal of Computer Vision (IJCV), vol. 130, pp. 2103-2130, 2022.</p>

    <p><a href="https://arxiv.org/abs/2201.10700"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Beyond Monocular Deraining: Parallel Stereo Deraining Network Via Semantic Prior,</p>

    <p>Kaihao Zhang+, Wenhan Luo#, Yanjiang Yu, Wenqi Ren, Fang Zhao, Changsheng Li, Lin Ma, Wei Liu, Hongdong Li,</p>

    <p>International Journal of Computer Vision (IJCV), vol. 130, pp. 1754-1769, 2022.</p>

    <p><a href="https://arxiv.org/abs/2105.03830"><strong>[PDF]</strong></a> <a href="https://github.com/HDCVLab/Stereo-Image-Deraining"><strong>[Github]</strong></a></p>
  </li>
  <li>
    <p>Aesthetic Text Logo Synthesis via Content-aware Layout Inferring,</p>

    <p>Yizhi Wang+, Guo Pu, Wenhan Luo, Yexin Wang, Pengfei Xiong, Hongwen Kang, Zhouhui Lian,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2022.</p>

    <p><a href="https://arxiv.org/abs/2204.02701"><strong>[PDF]</strong></a> <a href="https://github.com/yizhiwang96/TextLogoLayout"><strong>[Dataset/Code]</strong></a></p>
  </li>
  <li>
    <p>Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A Faster and Better Framework,</p>

    <p>Kaihao Zhang+, Dongxu Li, Wenhan Luo, Wenqi Ren, Wei Liu,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 45, pp. 1287-1293, 2023.</p>

    <p><a href="https://arxiv.org/abs/2103.12318"><strong>[arXiv]</strong></a> <a href="https://github.com/HDCVLab/Enhanced-Spatio-Temporal-Interaction-Learning-for-Video-Deraining"><strong>[Dataset/Code]</strong></a></p>
  </li>
  <li>
    <p>Benchmarking Ultra-High-Definition Image Super-resolution,</p>

    <p>Kaihao Zhang+, Dongxu Li, Wenhan Luo, Wenqi Ren, Bjorn Stenger, Wei Liu, Hongdong Li, Ming-Hsuan Yang,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), 2021.</p>

    <p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Benchmarking_Ultra-High-Definition_Image_Super-Resolution_ICCV_2021_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/HDCVLab/Benchmarking-Ultra-High-Definition-Image-Super-resolution"><strong>[Dataset]</strong></a></p>
  </li>
  <li>
    <p>Towards Distraction-Robust Active Visual Tracking,</p>

    <p>Fangwei Zhong+, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,</p>

    <p>International Conference on Machine Learning (ICML), 2021.</p>

    <p><a href="https://arxiv.org/abs/2106.10110"><strong>[PDF]</strong></a> <a href="https://github.com/zfw1226/active_tracking_rl/tree/distractor"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis,</p>

    <p>Wen Liu+, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, Shenghua Gao,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 44, pp. 5114-5132, 2022.</p>

    <p><a href="https://arxiv.org/abs/2011.09055"><strong>[PDF]</strong></a> <a href="https://github.com/iPERDance/iPERCore"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Multiple Object Tracking: A Literature Review,</p>

    <p>Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Tae-Kyun. Kim,</p>

    <p>Artificial Intelligence, vol. 293, pp. 103448, 2021.</p>

    <p><a href="https://www.sciencedirect.com/science/article/pii/S0004370220301958"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Every Moment Matters: Detail-Aware Networks to Bring a Blurry Image Alive,</p>

    <p>Kaihao Zhang+, Wenhan Luo, Bjorn Stenger, Wenqi Ren, Lin Ma, Hongdong Li,</p>

    <p>The 28th ACM International Conference on Multimedia (ACM MM), 2020. (<font color="red">Oral</font>)</p>

    <p><a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413929"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Beyond Monocular Deraining: Stereo Image Deraining via Semantic Understanding,</p>

    <p>Kaihao Zhang+, Wenhan Luo, Wenqi Ren, Jingwen Wang, Fang Zhao, Lin Ma, Hongdong Li,</p>

    <p>European Conference on Computer Vision (ECCV), UK, 2020.</p>

    <p><a href="https://link.springer.com/chapter/10.1007/978-3-030-58583-9_5"><strong>[PDF]</strong></a> <a href="https://pan.baidu.com/s/1T2UplwARbLS5apIQiAnEXg"><strong>[Dataset (zzkd)]</strong></a> <a href="https://pan.baidu.com/s/1cCqYg2Au8yhEXkypPhkszA"><strong>[Code (ehl2)]</strong></a> <a href="https://pan.baidu.com/s/1BV2-TPL5GiTlDbxjSR0qyg"><strong>[Results (yb4y)]</strong></a> <a href="https://github.com/HDCVLab/Beyond-Monocular-Deraining"><strong>[Github]</strong></a></p>
  </li>
  <li>
    <p>Deblurring by Realistic Blurring,</p>

    <p>Kaihao Zhang+, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, Hongdong Li,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2020. (<font color="red">Oral</font>)</p>

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deblurring_by_Realistic_Blurring_CVPR_2020_paper.pdf"><strong>[PDF]</strong></a> <a href="https://github.com/HDCVLab/Deblurring-by-Realistic-Blurring"><strong>[Dataset/Code]</strong></a></p>
  </li>
  <li>
    <p>Fine-grained Image-to-Image Transformation towards Visual Recognition,</p>

    <p>Wei Xiong, Yutong He, Yixuan Zhang, Wenhan Luo, Lin Ma, Jiebo Luo,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2020.</p>

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_Fine-Grained_Image-to-Image_Transformation_Towards_Visual_Recognition_CVPR_2020_paper.pdf"><strong>[PDF]</strong></a> <a href="https://wxiong.me/finegrain/"><strong>[Project Page]</strong></a></p>
  </li>
  <li>
    <p>AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking,</p>

    <p>Fangwei Zhong+, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 43, pp. 1467-1482, 2021.</p>

    <p><a href="https://ieeexplore.ieee.org/document/8896000"><strong>[PDF]</strong></a> <a href="https://github.com/zfw1226/active_tracking_rl"><strong>[Code]</strong></a> <a href="https://drive.google.com/file/d/1KKswxtaZlcHUYJmMEmAGnvXXrVxZ5rQQ/view"><strong>[Demo]</strong></a> <a href="https://github.com/zfw1226/gym-unrealcv"><strong>[Dataset]</strong></a></p>
  </li>
  <li>
    <p>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis,</p>

    <p>Wen Liu+, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao,</p>

    <p>Proc. of International Conference on Computer Vision (ICCV), Korea, 2019.</p>

    <p><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf"><strong>[PDF]</strong></a> <a href="https://svip-lab.github.io/project/impersonator.html"><strong>[Project Page]</strong></a> <a href="https://github.com/svip-lab/impersonator"><strong>[Code]</strong></a> <a href="https://svip-lab.github.io/dataset/iPER_dataset.html"><strong>[Dataset]</strong></a></p>
  </li>
  <li>
    <p>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video,</p>

    <p>Zhenfang Chen+, Lin Ma#, Wenhan Luo#, Kwan-Yee K Wong,</p>

    <p>The 57th Annual Meeting of the Association for Computational Linguistics (ACL), Italy, 2019. (<font color="red">Oral</font>)</p>

    <p><a href="https://aclanthology.org/P19-1183/"><strong>[PDF]</strong></a> <a href="https://github.com/zfchenUnique/WSSTG"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Face Anti-Spoofing: Model Matters, So Does Data,</p>

    <p>Xiao Yang*, Wenhan Luo*, Linchao Bao, Yuan Gao, Dihong Gong, Shibao Zheng, Zhifeng Li, Wei Liu,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019.</p>

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Face_Anti-Spoofing_Model_Matters_so_Does_Data_CVPR_2019_paper.pdf"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Learning Joint Gait Representation via Quintuplet Loss Minimization,</p>

    <p>Kaihao Zhang+, Wenhan Luo, Lin Ma, Wei Liu, Hongdong Li,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019. (<font color="red">Oral</font>)</p>

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Learning_Joint_Gait_Representation_via_Quintuplet_Loss_Minimization_CVPR_2019_paper.pdf"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Residual Regression with Semantic Prior for Crowd Counting,</p>

    <p>Jia Wan+, Wenhan Luo, Baoyuan Wu, Antoni Chan, Wei Liu,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019.</p>

    <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.pdf"><strong>[PDF]</strong></a> <a href="http://visal.cs.cityu.edu.hk/research/residual_regression_counting/"><strong>[Project Page]</strong></a> <a href="https://github.com/jia-wan/ResidualRegression-pytorch"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Learning to Compose Dynamic Tree Structures for Visual Contexts,</p>

    <p>Kaihua Tang+, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019. (<font color="red">Oral &amp; Best Paper Nominee</font>)</p>

    <p><a href="https://arxiv.org/abs/1812.01880"><strong>[arXiv]</strong></a> <a href="https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Bi-Real Net: Binarizing Deep Network towards Real-Network Performance,</p>

    <p>Zechun Liu+, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, Kwang-Ting Cheng,</p>

    <p>International Journal of Computer Vision (IJCV), vol. 128, pp. 202-219, 2020.</p>

    <p><a href="https://link.springer.com/epdf/10.1007/s11263-019-01227-8?author_access_token=NdkBwSULpW7AE0CplLNsQ_e4RwlQNchNByi7wbcMAY4zuJMqwdUG6xUI3neO68YE0HVQMhRVozqO0gNwgOgvcveiex4uWiK7xaxvf-r1oNmb1LdDYZoo4NSrjwdzGTAXBVNftZN-W65j0eIc1yxdag%3D%3D"><strong>[PDF]</strong></a> <a href="https://arxiv.org/abs/1811.01335"><strong>[arXiv]</strong></a> <a href="https://github.com/liuzechun/Bi-Real-net"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>AD-VAT: An Asymmetric Dueling Mechanism for Learning Visual Active Tracking,</p>

    <p>Fangwei Zhong+, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,</p>

    <p>International Conference on Learning Representations (ICLR), New Orleans, USA, 2019.</p>

    <p><a href="https://openreview.net/forum?id=HkgYmhR9KX"><strong>[OpenReview Link]</strong></a> <a href="https://github.com/zfw1226/active_tracking_rl"><strong>[Code]</strong></a> <a href="https://github.com/zfw1226/gym-unrealcv"><strong>[Dataset]</strong></a></p>
  </li>
  <li>
    <p>End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning,</p>

    <p>Wenhan Luo*, Peng Sun*, Fangwei Zhong*, Wei Liu, Tong Zhang, Yizhou Wang,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 42, pp. 1317-1332, 2020.</p>

    <p><a href="https://arxiv.org/abs/1808.03405"><strong>[arXiv]</strong></a> <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"><strong>[Project Page]</strong></a> <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>Cousin Network Guided Sketch Recognition via Latent Attribute Warehouse,</p>

    <p>Kaihao Zhang+, Wenhan Luo#, Lin Ma, Hongdong Li,</p>

    <p>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), Hawaii, USA, 2019. (<font color="red">Spotlight</font>)</p>

    <p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/4955"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm,</p>

    <p>Zechun Liu+, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng,</p>

    <p>European Conference on Computer Vision (ECCV), Germany, 2018.</p>

    <p><a href="https://arxiv.org/abs/1808.00278"><strong>[PDF]</strong></a> <a href="https://github.com/liuzechun/Bi-Real-net"><strong>[Code]</strong></a></p>
  </li>
  <li>
    <p>End-to-end Active Object Tracking via Reinforcement Learning,</p>

    <p>Wenhan Luo*, Peng Sun*, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang,</p>

    <p>International Conference on Machine Learning (ICML), Sweden, 2018.</p>

    <p><a href="https://arxiv.org/abs/1705.10561"><strong>[PDF]</strong></a> <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"><strong>[Project Page]</strong></a> <a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"><strong>[Code]</strong></a> <a href="https://drive.google.com/file/d/19Tz2rfF6i1CcTonOoS-xy1nIgx1qcQ9x/view"><strong>[Demo]</strong></a></p>
  </li>
  <li>
    <p>Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks,</p>

    <p>Wei Xiong+, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2018.</p>

    <p><a href="https://arxiv.org/abs/1709.07592"><strong>[arXiv]</strong></a> <a href="https://sites.google.com/site/whluoimperial/mdgan"><strong>[Project Page]</strong></a> <a href="https://github.com/weixiong-ur/mdgan"><strong>[Code]</strong></a> <a href="https://drive.google.com/file/d/1xWLiU-MBGN7MrsFHQm4_yXmfHBsMbJQo/view"><strong>[Dataset]</strong></a></p>
  </li>
  <li>
    <p>Real-Time Neural Style Transfer for Videos,</p>

    <p>Haozhi Huang+, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2017.</p>

    <p><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Automatic Topic Discovery for Multi-object Tracking,</p>

    <p>Wenhan Luo, Bjorn Stenger, Xiaowei Zhao, Tae-Kyun Kim,</p>

    <p>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), Austin, Texas, USA, 2015. (<font color="red">Oral</font>)</p>

    <p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/9789"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Bi-label Propagation for Generic Multiple Object Tracking,</p>

    <p>Wenhan Luo, Tae-Kyun Kim, Bjorn Stenger, Xiaowei Zhao, Roberto Cipolla,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, USA, 2014.</p>

    <p><a href="https://ieeexplore.ieee.org/document/6909564"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Unified Face Analysis by Iterative Multi-Output Random Forests,</p>

    <p>Xiaowei Zhao, Tae-Kyun Kim, Wenhan Luo,</p>

    <p>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, USA, 2014.</p>

    <p><a href="https://ieeexplore.ieee.org/document/6909624"><strong>[PDF]</strong></a></p>
  </li>
  <li>
    <p>Single and Multiple Object Tracking Using Log-Euclidean Riemannian Subspace and Block-Division Appearance Model,</p>

    <p>Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Steve Maybank, Zhongfei Zhang,</p>

    <p>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 34, no. 12, pp. 2420-2440, 2012.</p>

    <p><a href="http://ieeexplore.ieee.org/document/6143947/"><strong>[PDF]</strong></a></p>
  </li>
</ul>

<script>
function pubtoggleList() {
	var list = document.getElementById('pubhiddenList');
	list.style.display = list.style.display === 'none' ? 'block' : 'none';
	var button = document.getElementById('pubmore');
	button.style.display = button.style.display === 'none' ? 'block' : 'none';
	var buttom_less = document.getElementById('publess');
	buttom_less.style.display = buttom_less.style.display === 'none' ? 'block' : 'none';
}
</script>


          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/whluo/whluo.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
