<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenhan Luo - Publications</title>

  <meta name="author" content="Wenhan Luo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="icon" href="favicon.ico">
</head>

<body>


  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Wenhan Luo - List of Publications</name>
                  </p>
                  <p style="text-align:center">
                    (* indicates equal contribution, # indicates correspondence)
                  </p>


                </td>
              </tr>
            </tbody>
          </table>
             
     

           
            
            <br><br>
        </td>
      </tr>

                          

  </table>

  <table
  style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/uni-moe.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts</papertitle>,
        <br>Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, Min Zhang,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), to appear.</i>
        <br>
        [<a href="https://arxiv.org/abs/2405.11273" target="_blank">arXiv</a>]
        [<a href="https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs" target="_blank">Code</a>]
        [<a href="https://uni-moe.github.io/" target="_blank">Project Page</a>]
        [<a href="https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs?tab=readme-ov-file#%EF%B8%8F-uni-moe-weights" target="_blank">Model</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs?style=social">
        <p></p>   
      </td>
    </tr>
   
    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/era3d_neurips2024.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention</papertitle>,
        <br>Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, Yike Guo,<br>
        <i>Neural Information Processing Systems (NeurIPS), 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2405.11616" target="_blank">arXiv</a>]
        [<a href="https://github.com/pengHTYX/Era3D" target="_blank">Code</a>]
        [<a href="https://penghtyx.github.io/Era3D/" target="_blank">Project Page</a>]
        [<a href="https://huggingface.co/spaces/pengHTYX/Era3D_MV_demo" target="_blank">Hugging Face Demo</a>]
        [<a href="https://huggingface.co/pengHTYX/MacLab-Era3D-512-6view/tree/main" target="_blank">Model</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/pengHTYX/Era3D?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dsa_neurips2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models</papertitle>,
        <br>Lujun Li, Peijie Dong, Zhenheng Tang, Xiang Liu, Qiang Wang, Wenhan Luo, Wei Xue, Qifeng Liu, Xiaowen Chu, Yike Guo,<br>
        <i>Neural Information Processing Systems (NeurIPS), 2024.</i>
        <br>
        [<a href="https://openreview.net/pdf?id=rgtrYVC9n4" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dream_tkde2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>DREAM: Domain-agnostic Reverse Engineering Attributes of Black-box Model</papertitle>,
        <br>Rongqing Li, Jiaqi Yu, Changsheng Li, Wenhan Luo, Ye Yuan, Guoren Wang,<br>
        <i>IEEE Transactions on Knowledge and Data Engineering (TKDE), vol. 36, pp. 8009-8022, 2024.</i>
        <br>
        [<a href="https://www.computer.org/csdl/journal/tk/5555/01/10684294/20mFzDIL4OI" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/APPTrack+_ijcv2024.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>APPTracker+: Displacement Uncertainty for Occlusion Handling in Low-Frame-Rate Multiple Object Tracking</papertitle>,
                <br>Tao Zhou, Qi Ye, Wenhan Luo, Haizhou Ran, Zhiguo Shi, Jiming Chen,<br>
                <i>International Journal of Computer Vision (IJCV), to appear.</i>
                <br>
                [<a href="https://link.springer.com/article/10.1007/s11263-024-02237-x" target="_blank">PDF</a>]
                <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/DTDA_tcsvt2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing</papertitle>,
                <br>Zhe Kong, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo#,<br>
                <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 13177-13189, 2024.</i>
                <br>
                [<a href="https://arxiv.org/abs/2401.01102" target="_blank">PDF</a>]
                <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/StableBFVR_acmmm2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Blind Face Video Restoration with Temporal Consistent Generative Prior and Degradation-Aware Prompt</papertitle>,
                <br>Jingfan Tan, Hyunhee Park, Ying Zhang, Tao Wang, Kaihao Zhang, Xiangyu Kong, Pengwen Dai, Zikun Liu, Wenhan Luo#,<br>
                <i>The 32rd ACM International Conference on Multimedia (ACM MM), 2024.</i>
                <br>
                [<a href="https://dl.acm.org/doi/10.1145/3664647.3680917" target="_blank">PDF</a>]
                <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/omg_eccv2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>OMG: Occlusion-friendly Personalized Multi-concept Generation In Diffusion Models</papertitle>,
        <br>Zhe Kong, Yong Zhang#, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, Wenhan Luo#,<br>
        <i>European Conference on Computer Vision (ECCV), 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2403.10983" target="_blank">PDF</a>]
        [<a href="https://github.com/kongzhecn/OMG" target="_blank">Code</a>]
        [<a href="https://kongzhecn.github.io/omg-project/" target="_blank">Project Page</a>]
        [<a href="https://huggingface.co/spaces/Fucius/OMG" target="_blank">Hugging Face (OMG+LoRAs)</a>]
        [<a href="https://huggingface.co/spaces/Fucius/OMG-InstantID" target="_blank">Hugging Face (OMG+InstantID)</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/kongzhecn/OMG?style=social">
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/PromptFDDM_eccv2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Prompting Future Driven Diffusion Model for Hand Motion Prediction</papertitle>,
        <br>Bowen Tang, Kaihao Zhang#, Wenhan Luo#, Wei Liu, Hongdong Li,<br>
        <i>European Conference on Computer Vision (ECCV), 2024.</i>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01102.pdf" target="_blank">PDF</a>]
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/autogas_eccv2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search</papertitle>,
        <br>Lujun Li, Haosen Sun, Shiwen Li, Peijie Dong, Wenhan Luo, Wei Xue, Qifeng Liu, Yike Guo,<br>
        <i>European Conference on Computer Vision (ECCV), 2024.</i>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00668.pdf" target="_blank">PDF</a>]
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/attnzero_eccv2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>AttnZero: Efficient Attention Discovery for Vision Transformers</papertitle>,
        <br>Lujun Li, Zimian Wei, Peijie Dong, Wenhan Luo, Wei Xue, Qifeng Liu, Yike Guo,<br>
        <i>European Conference on Computer Vision (ECCV), 2024.</i>
        <br>
        [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00666.pdf" target="_blank">PDF</a>]
        <p></p>  
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/detkds_icml2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>DetKDS: Knowledge Distillation Search for Object Detectors</papertitle>,
        <br>Lujun Li, Yufan Bao, Peijie Dong, Chuanguang Yang, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo,<br>
        <i>International Conference on Machine Learning (ICML), 2024.</i>
        <br>
        [<a href="https://openreview.net/pdf/fcca99569d2c5ab0653814d94a6625b2e27ef2a2.pdf" target="_blank">PDF</a>]
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dempaa_tgrs2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>DeMPAA: Deployable Multi-Mini-Patch Adversarial Attack for Remote Sensing Image Classification</papertitle>,
        <br>Jun-Jie Huang, Ziyue Wang, Tianrui Liu, Wenhan Luo, Zihan Chen, Wentao Zhao, Meng Wang,<br>
        <i>IEEE Trans. on Geoscience and Remote Sensing, vol. 62, pp. 1-13, 2024.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/10521508" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/BFRffusion_tcsvt2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Towards Real-World Blind Face Restoration with Generative Diffusion Prior</papertitle>,
                <br>Xiaoxu Chen, Jingfan Tan, Tao Wang, Kaihao Zhang, Wenhan Luo#, Xiaochun Cao,<br>
                <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 8494-8508, 2024.</i>
                <br>
                [<a href="https://arxiv.org/abs/2312.15736" target="_blank">PDF</a>]
                [<a href="https://github.com/chenxx89/BFRffusion" target="_blank">Code</a>]
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/chenxx89/BFRffusion?style=social">
                <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/gridformer_ijcv2024.PNG' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions</papertitle>,
        <br>Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo#, Bjorn Stenger, Tong Lu, Tae-Kyun Kim, Wei Liu,
        Hongdong Li,<br>
        <i>International Journal of Computer Vision (IJCV), vol. 132, pp. 4541-4563, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2305.17863" target="_blank">PDF</a>]
        [<a href="https://github.com/TaoWangzj/GridFormer" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TaoWangzj/GridFormer?style=social">
        <p></p> 
      </td>
    </tr>



    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/qi_cvpr2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</papertitle>,
        <br>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue,
        Shanghang Zhang, Qifeng Liu, Yike Guo,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2311.17532" target="_blank">PDF</a>]
        [<a href="https://xingqunqi-lab.github.io/Emo-Transition-Gesture/" target="_blank">Project Page</a>]
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/querynlt_cvpr2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Context-Aware Integration of Language and Visual References for Natural Language Tracking</papertitle>,
        <br>Yanyan Shao, Shuting He, Qi Ye, Yuchao Feng, Wenhan Luo, Jiming Chen,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2403.19975" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/modict_coling2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation</papertitle>,
        <br>Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang,<br>
        <i>LREC-COLING, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2402.13587" target="_blank">PDF</a>]
        [<a href="https://github.com/HITsz-TMG/Multimodal-In-Context-Tuning/" target="_blank">Dataset</a>]
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/aux-nas_iclr2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost</papertitle>,
        <br>Yuan Gao, Weizhong Zhang, Wenhan Luo, Lin Ma, Jin-Gang Yu, Gui-Song Xia, Jiayi Ma,<br>
        <i>International Conference on Learning Representations (ICLR), 2024.</i>
        <br>
        [<a href="https://openreview.net/forum?id=cINwAhrgLf&noteId=cINwAhrgLf" target="_blank">PDF</a>]
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mfpsnet_tnnls2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Multi-Prior Learning via Neural Architecture Search for Blind Face Restoration</papertitle>,
                <br>Yanjiang Yu, Puyang Zhang, Kaihao Zhang, Wenhan Luo, Changsheng Li, Ye Yuan, Guoren Wang,<br>
                <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), to appear.</i>
                <br>
                [<a href="https://ieeexplore.ieee.org/document/10355907" target="_blank">PDF</a>]
                [<a href="https://github.com/YYJ1anG/MFPSNet" target="_blank">Code</a>]
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/YYJ1anG/MFPSNet?style=social">
                <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/udc_tcsvt2024.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</papertitle>,
                <br>Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo#, Xiaochun Cao,<br>
                <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 4914-4927, 2024.</i>
                <br>
                [<a href="https://arxiv.org/abs/2308.10196" target="_blank">PDF</a>]
                <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/punctuation_attack_neurips2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models</papertitle>,
        <br>Wenqiang Wang, Chongyang Du, Tao Wang, Kaihao Zhang, Wenhan Luo#, Lin Ma, Wei Liu, Xiaochun Cao,<br>
        <i>Neural Information Processing Systems (NeurIPS), 2023.</i>
        <br>
        [<a href="https://openreview.net/pdf?id=ir6WWkFR80" target="_blank">PDF</a>]
        <p></p> 
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mc-blur_tcsvt2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>MC-Blur: A Comprehensive Benchmark for Image Deblurring</papertitle>,
        <br>Kaihao Zhang, Tao Wang, Wenhan Luo#, Wenqi Ren, Bjorn Stenger, Wei Liu, Hongdong Li, Ming-Hsuan
        Yang,<br>
        <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 3755-3767, 2024.</i>
        (<strong><font color="red">Highly Cited Paper</font> </strong>)</em>
        <br>
        [<a href="https://arxiv.org/abs/2112.00234" target="_blank">PDF</a>]
        [<a href="https://github.com/HDCVLab/MC-Blur-Dataset" target="_blank">Dataset</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/MC-Blur-Dataset?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/restoration_vision_pr2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Restoring Vision in Hazy Weather with Hierarchical Contrastive Learning</papertitle>,
        <br>Tao Wang, Guangpin Tao, Wanglong Lu, Kaihao Zhang, Wenhan Luo, Xiaoqin Zhang, Tong Lu,<br>
        <i>Pattern Recognition, vol. 145, pp. 109956, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2212.11473" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/airformer_tcsvt2024.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Frequency-oriented Efficient Transformer for All-in-one Weather-degraded Image Restoration</papertitle>,
        <br>Tao Gao, Yuanbo Wen, Kaihao Zhang, Jing Zhang, Ting Chen, Lidong Liu, Wenhan Luo,<br>
        <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 1886-1899, 2024.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/10196308" target="_blank">PDF</a>]
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/FnFAttack_iccv2023.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>FnF Attack: Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives</papertitle>,
        <br>Tao Zhou, Qi Ye#, Wenhan Luo#, Kaihao Zhang, Zhiguo Shi, Jiming Chen,<br>
        <i>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_FF_Attack_Adversarial_Attack_against_Multiple_Object_Trackers_by_Inducing_ICCV_2023_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://infzhou.github.io/FnFAttack/index.html" target="_blank">Project Page</a>]
        [<a href="https://github.com/infZhou/FnF_Attack" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/infZhou/FnF_Attack?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/prior_iccv2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>PRIOR: Prototype Representation Joint Learning from Medical Images and Reports</papertitle>,
        <br>Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang,<br>
        <i>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_PRIOR_Prototype_Representation_Joint_Learning_from_Medical_Images_and_Reports_ICCV_2023_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://github.com/QtacierP/PRIOR" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/QtacierP/PRIOR?style=social">
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mbtaylor_iccv2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>MB-TaylorFormer: Mutil-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing</papertitle>,
        <br>Yuwei Qiu, Kaihao Zhang, Chenxi Wang, Wenhan Luo, Hongdong Li, Zhi Jin,<br>
        <i>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_MB-TaylorFormer_Multi-Branch_Efficient_Transformer_Expanded_by_Taylor_Formula_for_Image_ICCV_2023_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/FVL2020/ICCV-2023-MB-TaylorFormer?style=social">
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/homography_iccv2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Homography Guided Temporal Fusion for Road Line and Marking Segmentation</papertitle>,
        <br>Shan Wang, Chuong Nguyen, Jiawei Liu, Kaihao Zhang, Wenhan Luo, Yanhao Zhang, Sundaram Muthu, Fahira
        Afzal Maken, Hongdong Li,<br>
        <i>Proc. of International Conference on Computer Vision (ICCV), Paris, France, 2023.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://github.com/ShanWang-Shan/HomoFusion" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ShanWang-Shan/HomoFusion?style=social">
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/tio_iros2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild</papertitle>,
        <br>Yanyan Shao, Qi Ye, Wenhan Luo, Kaihao Zhang, Jiming Chen,<br>
        <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023.</i>
        <br>
        [<a href="https://arxiv.org/abs/2308.03061" target="_blank">PDF</a>]
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/robust_sirr_cvpr2023.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Robust Single Image Reflection Removal Against Adversarial Attacks</papertitle>,
        <br>Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo#, Zhaoxin Fan, Wenqi Ren, Jianfeng Lu,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2023.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://github.com/ZhenboSong/RobustSIRR" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ZhenboSong/RobustSIRR?style=social">
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dfdm_tnnls2024.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Taming Self-Supervised Learning for Presentation Attack Detection: De-Folding and De-Mixing</papertitle>,
        <br>Zhe Kong, Wentian Zhang, Feng Liu, Wenhan Luo, Haozhe Liu, Linlin Shen, Raghavendra Ramachandra,<br>
        <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), vol. 35, pp. 10639-10650, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2109.04100" target="_blank">PDF</a>]
        [<a href="https://github.com/kongzhecn/dfdm" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/kongzhecn/dfdm?style=social">
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/llformer_aaai2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Benchmarking Ultra-High-Definition Low-Light Image Enhancement and A Transformer Method</papertitle>,
        <br>Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo#, Bjorn Stenger, Tong Lu#,<br>
        <i>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), USA, 2023. (<strong>
            <font color="red">Oral</font>
          </strong>)</i>
        <br>
        [<a href="https://arxiv.org/abs/2212.11548" target="_blank">PDF</a>]
        [<a href="https://github.com/TaoWangzj/LLFormer" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TaoWangzj/LLFormer?style=social">
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/clip2video_tmm2023.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Transferring Image-CLIP to Video-Text Retrieval via Temporal Relations</papertitle>,
        <br>Han Fang, Pengfei Xiong, Luhui Xu, Wenhan Luo#,<br>
        <i>IEEE Transactions on Multimedia (TMM), vol. 25, pp. 7772-7785, 2023.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9973385" target="_blank">PDF</a>]
        [<a href="https://github.com/CryhanFang/CLIP2Video" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CryhanFang/CLIP2Video?style=social">
        <p></p>   
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/t-net_tmm2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>T-Net: Deep Stacked Scale-iteration Network for Image Dehazing</papertitle>,
        <br>Lirong Zheng, Yanshan Li, Kaihao Zhang, Wenhan Luo,<br>
        <i>IEEE Transactions on Multimedia (TMM), vol. 25, pp. 6794-6807, 2023.</i>
        <br>
        [<a href="https://arxiv.org/abs/2106.02809" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/safecount_wacv2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Few-shot Object Counting with Similarity-Aware Feature Enhancement</papertitle>,
        <br>Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le,<br>
        <i>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023. (<strong>
            <font color="red">Oral</font>
          </strong>)</i>
        <br>
        [<a href="https://arxiv.org/abs/2201.08959" target="_blank">PDF</a>]
        [<a href="https://github.com/zhiyuanyou/SAFECount" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zhiyuanyou/SAFECount?style=social">
        <p></p>  
      </td>
    </tr>



    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/APPTrack_acmmm2022.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>APPTracker: Improving Tracking Multiple Objects in Low-Frame-Rate Videos</papertitle>,
        <br>Tao Zhou, Wenhan Luo, Zhiguo Shi, Jiming Chen, Qi Ye,<br>
        <i>The 30th ACM International Conference on Multimedia (ACM MM), 2022.</i>
        <br>
        [<a
          href="https://infzhou.github.io/folder/Zhou_APPTracker_Improving_Tracking_Multiple_Objects_in_Low-Frame-Rate_Videos_MM_2022.pdf"
          target="_blank">PDF</a>]
        [<a href="https://infzhou.github.io/appTracker/index.html" target="_blank">Project Page</a>]
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/edface_tpami2023.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset</papertitle>,
        <br>Kaihao Zhang, Dongxu Li, Wenhan Luo, Jingyu Liu, Jiankang Deng, Wei Liu, Stefanos Zafeiriou,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 45, pp. 3968-3978, 2023.</i>
        <br>
        [<a href="https://arxiv.org/abs/2110.05031" target="_blank">PDF</a>]
        [<a href="https://github.com/HDCVLab/EDFace-Celeb-1M" target="_blank">Github</a>]
        [<a href="https://zhangkaihao.github.io/projects/EDface/" target="_blank">Project Page</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/EDFace-Celeb-1M?style=social">

        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/image_survey_ijcv2022.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Deep Image Deblurring: A Survey</papertitle>,
        <br>
        Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bjorn Stenger, Ming-Hsuan Yang, Hongdong Li,
        <br>
        <em>International Journal of Computer Vision (IJCV), vol. 130, pp. 2103-2130, 2022.</em>
        (<strong><font color="red">Highly Cited Paper</font> </strong>)</em>
        <br>
        [<a href="https://arxiv.org/abs/2201.10700" target="_blank">PDF</a>]
        <p></p>  
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/eprrnet_ijcv2022.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Beyond Monocular Deraining: Parallel Stereo Deraining Network Via Semantic Prior</papertitle>,
        <br>
        Kaihao Zhang, Wenhan Luo#, Yanjiang Yu, Wenqi Ren, Fang Zhao, Changsheng Li, Lin Ma, Wei Liu, Hongdong
        Li,
        <br>
        <em>International Journal of Computer Vision (IJCV), vol. 130, pp. 1754-1769, 2022.</em>
        <br>
        [<a href="https://arxiv.org/abs/2105.03830" target="_blank">PDF</a>]
        [<a href="https://github.com/HDCVLab/Stereo-Image-Deraining" target="_blank">Github</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Stereo-Image-Deraining?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/textlogolayout_cvpr2022.svg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Aesthetic Text Logo Synthesis via Content-aware Layout Inferring</papertitle>,
        <br>Yizhi Wang, Guo Pu, Wenhan Luo, Yexin Wang, Pengfei Xiong, Hongwen Kang, Zhouhui Lian,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2022.</i>
        <br>
        [<a href="https://arxiv.org/abs/2204.02701" target="_blank">PDF</a>]
        [<a href="https://github.com/yizhiwang96/TextLogoLayout" target="_blank">Dataset/Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/yizhiwang96/TextLogoLayout?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/estinet_tpami2023.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A Faster and Better Framework</papertitle>,
        <br>Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Wei Liu,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 45, pp. 1287-1293, 2023.</i>
        <br>
        [<a href="https://arxiv.org/abs/2103.12318" target="_blank">arXiv</a>]
        [<a href="https://github.com/HDCVLab/Enhanced-Spatio-Temporal-Interaction-Learning-for-Video-Deraining"
          target="_blank">Dataset/Code</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Enhanced-Spatio-Temporal-Interaction-Learning-for-Video-Deraining?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/daiam_tip2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop Removal</papertitle>,
        <br>Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 30, pp. 7608-7619, 2021.</i>
        <br>
        [<a href="https://arxiv.org/pdf/2103.07051.pdf" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/ddmsnet_tip2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Deep Dense Multi-scale Network for Snow Removal Using Semantic and Geometric Priors</papertitle>,
        <br>Kaihao Zhang, Rongqing Li, Yanjiang Yu, Wenhan Luo, Changsheng Li,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 30, pp. 7419-7431, 2021.</i>
        <br>
        [<a href="https://arxiv.org/abs/2103.11298" target="_blank">PDF</a>]
        [<a href="https://github.com/HDCVLab/Deep-Dense-Multi-scale-Network-for-Snow-Removal"
          target="_blank">Dataset/Code</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Deep-Dense-Multi-scale-Network-for-Snow-Removal?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/benchmarksr_iccv2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Benchmarking Ultra-High-Definition Image Super-resolution</papertitle>,
        <br>Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Bjorn Stenger, Wei Liu, Hongdong Li, Ming-Hsuan
        Yang,<br>
        <i>Proc. of International Conference on Computer Vision (ICCV), 2021.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Benchmarking_Ultra-High-Definition_Image_Super-Resolution_ICCV_2021_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://github.com/HDCVLab/Benchmarking-Ultra-High-Definition-Image-Super-resolution"
          target="_blank">Dataset</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Benchmarking-Ultra-High-Definition-Image-Super-resolution?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/deblursr_tip2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding</papertitle>,
        <br>Wenjia Niu, Kaihao Zhang, Wenhan Luo#, Yiran Zhong,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 30, pp. 7101-7111, 2021.</i>
        <br>
        [<a href="https://arxiv.org/abs/2105.13077" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/laga-net_tmm2022.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>LAGA-Net: Local-And-Global Attention Network for Skeleton Based Action Recognition</papertitle>,
        <br>Rongjie Xia, Yanshan Li, Wenhan Luo,<br>
        <i>IEEE Transactions on Multimedia (TMM), vol. 24, pp. 2648-2661, 2022.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/abstract/document/9447926" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dr_vat_icml2021.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Towards Distraction-Robust Active Visual Tracking</papertitle>,
        <br>Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,<br>
        <i>International Conference on Machine Learning (ICML), 2021.</i>
        <br>
        [<a href="https://arxiv.org/abs/2106.10110" target="_blank">PDF</a>]
        [<a href="https://github.com/zfw1226/active_tracking_rl/tree/distractor" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social">
        [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Environment</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dprn_tip2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Single Image Dehazing via Dual-Path Recurrent Network</papertitle>,
        <br>Xiaoqin Zhang, Runhua Jiang, Tao Wang, Wenhan Luo,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 30, pp. 5211-5222, 2021.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9435998" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/liquid_tpami2022.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis</papertitle>,
        <br>Wen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, Shenghua Gao,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 44, pp. 5114-5132, 2022.</i>
        <br>
        [<a href="https://arxiv.org/abs/2011.09055" target="_blank">PDF</a>]
        [<a href="https://github.com/iPERDance/iPERCore" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/iPERDance/iPERCore?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mdlbp_tgrs2022.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Multidimensional Local Binary Pattern for Hyperspectral Image Classification</papertitle>,
        <br>Yanshan Li, Haojin Tang, Weixin Xie, Wenhan Luo,<br>
        <i>IEEE Trans. on Geoscience and Remote Sensing, vol. 60, pp. 1-13, 2022.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9400381" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dfn_tmm2022.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Disentangled Feature Networks for Facial Portraits Generation</papertitle>,
        <br>Kaihao Zhang, Wenhan Luo#, Lin Ma, Wenqi Ren, Hongdong Li,<br>
        <i>IEEE Transactions on Multimedia (TMM), vol. 24, pp. 1378-1388, 2022.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9372827" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mot_survey_ai2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Multiple Object Tracking: A Literature Review</papertitle>,
        <br>Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Tae-Kyun. Kim,<br>
        <i>Artificial Intelligence, vol. 293, pp. 103448, 2021.</i>
        (<strong><font color="red">Highly Cited Paper</font> </strong>)</em>
        <br>
        [<a href="https://www.sciencedirect.com/science/article/pii/S0004370220301958" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/mfm_tcsvt2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Multi-level Fusion and Attention-guided CNN for Image Dehazing</papertitle>,
        <br>Xiaoqin Zhang, Tao Wang, Wenhan Luo, Pengcheng Huang,<br>
        <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 31, pp. 4162-4173, 2021. </i>
        (<strong><font color="red">Highly Cited Paper</font> </strong>)</em>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9302656" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/gmlfe_tip2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling</papertitle>,
        <br>Tianrui Liu, Wenhan Luo, Lin Ma, Junjie Huang, Tania Stathaki, Tianhong Dai,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 30, pp. 754-766, 2021.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9271862" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/self_flow_tip2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>STFlow: Self-Taught Optical Flow Estimation Using Pseudo Labels</papertitle>,
        <br>Zhe Ren, Wenhan Luo, Junchi Yan, Xiaokang Yang, Alan Yuille, Hongyuan Zha,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 29, pp. 9113-9124, 2020.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/9201360" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/heptagan_acmmm2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Every Moment Matters: Detail-Aware Networks to Bring a Blurry Image Alive</papertitle>,
        <br>Kaihao Zhang, Wenhan Luo, Bjorn Stenger, Wenqi Ren, Lin Ma, Hongdong Li<br><em>The 28th ACM
          International Conference on Multimedia (ACM MM), 2020. (<strong>
            <font color="red">Oral</font>
          </strong>)</em><br>
        [<a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413929" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/da_mot_pr2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Distractor-Aware Discrimination Learning for Online Multiple Object Tracking</papertitle>,
        <br>Zongwei Zhou, Wenhan Luo, Qiang Wang, Junliang Xing, Weiming Hu,<br>
        <i>Pattern Recognition, vol. 107, pp. 107512, 2020.</i>
        <br>
        [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320303150"
          target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/prrnet_eccv2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Beyond Monocular Deraining: Stereo Image Deraining via Semantic Understanding</papertitle>,
        <br>
        Kaihao Zhang, Wenhan Luo, Wenqi Ren, Jingwen Wang, Fang Zhao, Lin Ma, Hongdong Li,
        <br>
        <em>European Conference on Computer Vision (ECCV), UK, 2020.</em>
        <br>
        [<a href="https://link.springer.com/chapter/10.1007/978-3-030-58583-9_5" target="_blank">PDF</a>]
        [<a href="https://pan.baidu.com/s/1T2UplwARbLS5apIQiAnEXg" target="_blank">Dataset (zzkd)</a>]
        [<a href="https://pan.baidu.com/s/1cCqYg2Au8yhEXkypPhkszA" target="_blank">Code (ehl2)</a>]
        [<a href="https://pan.baidu.com/s/1BV2-TPL5GiTlDbxjSR0qyg" target="_blank">Results (yb4y)</a>]
        [<a href="https://github.com/HDCVLab/Beyond-Monocular-Deraining" target="_blank">Github</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Beyond-Monocular-Deraining?style=social">
        <p></p>    
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/dbgan_cvpr2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Deblurring by Realistic Blurring</papertitle>,
                <br>
                Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, Hongdong Li,
                <br>
                <em>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2020. (<strong>
                    <font color="red">Oral</font>
                  </strong>)</em>
                <br>
                [<a
                  href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deblurring_by_Realistic_Blurring_CVPR_2020_paper.pdf"
                  target="_blank">PDF</a>]
                [<a href="https://github.com/HDCVLab/Deblurring-by-Realistic-Blurring" target="_blank">Dataset/Code</a>]
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HDCVLab/Deblurring-by-Realistic-Blurring?style=social">
                <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/i2itrans_cvpr2020.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Fine-grained Image-to-Image Transformation towards Visual Recognition</papertitle>,
        <br>
        Wei Xiong, Yutong He, Yixuan Zhang, Wenhan Luo, Lin Ma, Jiebo Luo,
        <br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2020.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xiong_Fine-Grained_Image-to-Image_Transformation_Towards_Visual_Recognition_CVPR_2020_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://wxiong.me/finegrain/" target="_blank">Project Page</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/spgan_cviu2021.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Video Deblurring via Spatiotemporal Pyramid Network and Adversarial Gradient Prior</papertitle>,
        <br>Tao Wang, Xiaoqin Zhang, Runhua Jiang, Li Zhao, Huiling Chen, Wenhan Luo,<br>
        <i>Computer Vision and Image Understanding (CVIU), vol. 203, pp. 103135, 2021.</i>
        <br>
        [<a href="https://www.sciencedirect.com/science/article/pii/S1077314220301545" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/advatplus_tpami2021.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking</papertitle>,
        <br>Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 43, pp. 1467-1482, 2021.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/8896000" target="_blank">PDF</a>]
        [<a href="https://github.com/zfw1226/active_tracking_rl" target="_blank">Code</a>]
        [<a href="https://drive.google.com/file/d/1KKswxtaZlcHUYJmMEmAGnvXXrVxZ5rQQ/view"
          target="_blank">Demo</a>]
        [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Dataset</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social">
        [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Environment</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social">
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:justify">
        <img src='./multimedia/liquidgan_iccv2019.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis</papertitle>,
        <br>
        Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao,
        <br>
        <i>Proc. of International Conference on Computer Vision (ICCV), Korea, 2019.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="https://svip-lab.github.io/project/impersonator.html" target="_blank">Project Page</a>]
        [<a href="https://github.com/svip-lab/impersonator" target="_blank">Code</a>]
        [<a href="https://svip-lab.github.io/dataset/iPER_dataset.html" target="_blank">Dataset</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/svip-lab/impersonator?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/wsstg_acl2019.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</papertitle>,
        <br>
        Zhenfang Chen, Lin Ma#, Wenhan Luo#, Kwan-Yee K Wong,
        <br>
        <i>The 57th Annual Meeting of the Association for Computational Linguistics (ACL), Italy, 2019.
          (<strong>
            <font color="red">Oral</font>
          </strong>)</i>
        <br>
        [<a href="https://aclanthology.org/P19-1183/" target="_blank">PDF</a>]
        [<a href="https://github.com/zfchenUnique/WSSTG" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfchenUnique/WSSTG?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/stasn_cvpr2019.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Face Anti-Spoofing: Model Matters, So Does Data</papertitle>,
        <br>
        Xiao Yang*, Wenhan Luo*, Linchao Bao, Yuan Gao, Dihong Gong, Shibao Zheng, Zhifeng Li, Wei Liu,
        <br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Face_Anti-Spoofing_Model_Matters_so_Does_Data_CVPR_2019_paper.pdf"
          target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/gait_cvpr2019.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Learning Joint Gait Representation via Quintuplet Loss Minimization</papertitle>,
        <br>
        Kaihao Zhang, Wenhan Luo, Lin Ma, Wei Liu, Hongdong Li,
        <br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019. (<strong>
            <font color="red">Oral</font>
          </strong>)</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Learning_Joint_Gait_Representation_via_Quintuplet_Loss_Minimization_CVPR_2019_paper.pdf"
          target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/res_count_cvpr2019.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Residual Regression with Semantic Prior for Crowd Counting</papertitle>,
        <br>Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni Chan, Wei Liu,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.pdf"
          target="_blank">PDF</a>]
        [<a href="http://visal.cs.cityu.edu.hk/research/residual_regression_counting/" target="_blank">Project
          Page</a>]
        [<a href="https://github.com/jia-wan/ResidualRegression-pytorch" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/jia-wan/ResidualRegression-pytorch?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/vctree_cvpr2019.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Learning to Compose Dynamic Tree Structures for Visual Contexts</papertitle>,
        <br>Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019. (<strong>
            <font color="red">Oral & Best Paper Nominee</font>
          </strong>)</i>
        <br>
        [<a href="https://arxiv.org/abs/1812.01880" target="_blank">arXiv</a>]
        [<a href="https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/KaihuaTang/VCTree-Scene-Graph-Generation?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/bireal-ijcv2020.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Bi-Real Net: Binarizing Deep Network towards Real-Network Performance</papertitle>,
        <br>Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, Kwang-Ting Cheng,<br>
        <i>International Journal of Computer Vision (IJCV), vol. 128, pp. 202-219, 2020.</i>
        <br>
        [<a
          href="https://link.springer.com/epdf/10.1007/s11263-019-01227-8?author_access_token=NdkBwSULpW7AE0CplLNsQ_e4RwlQNchNByi7wbcMAY4zuJMqwdUG6xUI3neO68YE0HVQMhRVozqO0gNwgOgvcveiex4uWiK7xaxvf-r1oNmb1LdDYZoo4NSrjwdzGTAXBVNftZN-W65j0eIc1yxdag%3D%3D"
          target="_blank">PDF</a>]
        [<a href="https://arxiv.org/abs/1811.01335" target="_blank">arXiv</a>]
        [<a href="https://github.com/liuzechun/Bi-Real-net" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/liuzechun/Bi-Real-net?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/advat_iclr2019.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>AD-VAT: An Asymmetric Dueling Mechanism for Learning Visual Active Tracking</papertitle>,
        <br>Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang,<br>
        <i>International Conference on Learning Representations (ICLR), New Orleans, USA, 2019.</i>
        <br>
        [<a href="https://openreview.net/forum?id=HkgYmhR9KX" target="_blank">OpenReview Link</a>]
        [<a href="https://github.com/zfw1226/active_tracking_rl" target="_blank">Code</a>]
        [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Dataset</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social">
        [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Environment</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/e2e-real_tpami2020.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning</papertitle>,
        <br>Wenhan Luo*, Peng Sun*, Fangwei Zhong*, Wei Liu, Tong Zhang, Yizhou Wang,<br>
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 42, pp. 1317-1332, 2020.</i>
        <br>
        [<a href="https://arxiv.org/abs/1808.03405" target="_blank">arXiv</a>]
        [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Project
          Page</a>]
        [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"
          target="_blank">Code</a>]
          [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Environment</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/warehouse_aaai2019.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Cousin Network Guided Sketch Recognition via Latent Attribute Warehouse</papertitle>,
        <br>Kaihao Zhang, Wenhan Luo#, Lin Ma, Hongdong Li,<br>
        <i>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), Hawaii, USA, 2019.
          (<strong>
            <font color="red">Spotlight</font>
          </strong>)</i>
        <br>
        [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/4955" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/dblrgan_tip2019.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Adversarial Spatio-Temporal Learning for Video Deblurring</papertitle>,
        <br>Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu, Hongdong Li,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 28, no. 1, pp. 291-301, 2019.</i>
        <br>
        [<a href="https://arxiv.org/abs/1804.00533" target="_blank">arXiv</a>]
        [<a href="https://drive.google.com/file/d/1lBA-R32EYKsVn7ZYcfn1ibO8UdLFH_yI/view"
          target="_blank">Code</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/trajastopic_tip2019.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Trajectories as Topics: Multi-Object Tracking by Topic Discovery</papertitle>,
        <br>Wenhan Luo, Bjorn Stenger, Xiaowei Zhao, Tae-Kyun Kim,<br>
        <i>IEEE Trans. on Image Processing (TIP), vol. 28, no. 1, pp. 240-252, 2019.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/8444759" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/bireal-eccv2018.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm</papertitle>,
        <br>Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng,<br>
        <i>European Conference on Computer Vision (ECCV), Germany, 2018.</i>
        <br>
        [<a href="https://arxiv.org/abs/1808.00278" target="_blank">PDF</a>]
        [<a href="https://github.com/liuzechun/Bi-Real-net" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/liuzechun/Bi-Real-net?style=social">
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/e2e_tracking_icml18.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>End-to-end Active Object Tracking via Reinforcement Learning</papertitle>,
        <br>Wenhan Luo*, Peng Sun*, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang,<br>
        <i>International Conference on Machine Learning (ICML), Sweden, 2018.</i>
        <br>
        [<a href="https://arxiv.org/abs/1705.10561" target="_blank">PDF</a>]
        [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Project
          Page</a>]
        [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018"
          target="_blank">Code</a>]
        [<a href="https://drive.google.com/file/d/19Tz2rfF6i1CcTonOoS-xy1nIgx1qcQ9x/view"
          target="_blank">Demo</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/mdgan_cvpr2018.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</papertitle>,
        <br>Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2018.</i>
        <br>
        [<a href="https://arxiv.org/abs/1709.07592" target="_blank">arXiv</a>]
        [<a href="https://sites.google.com/site/whluoimperial/mdgan" target="_blank">Project Page</a>]
        [<a href="https://github.com/weixiong-ur/mdgan" target="_blank">Code</a>]
        [<a href="https://drive.google.com/file/d/1xWLiU-MBGN7MrsFHQm4_yXmfHBsMbJQo/view"
          target="_blank">Dataset</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/weixiong-ur/mdgan?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/video_style_cvpr2017.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Real-Time Neural Style Transfer for Videos</papertitle>,
        <br>Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2017.</i>
        <br>
        [<a
          href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf"
          target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:30%;vertical-align:middle;text-align:center">
        <img src='./multimedia/topic_model_aaai2015.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:70%;vertical-align:middle;text-align:justify">
        <papertitle>Automatic Topic Discovery for Multi-object Tracking</papertitle>,
        <br>Wenhan Luo, Bjorn Stenger, Xiaowei Zhao, Tae-Kyun Kim,<br>
        <i>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), Austin, Texas, USA,
          2015. (<strong>
            <font color="red">Oral</font>
          </strong>)</i>
        <br>
        [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/9789" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/bi-label_cvpr2014.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Bi-label Propagation for Generic Multiple Object Tracking</papertitle>,
        <br>Wenhan Luo, Tae-Kyun Kim, Bjorn Stenger, Xiaowei Zhao, Roberto Cipolla,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, USA, 2014.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/6909564" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/facial_analysis_cvpr2014.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Unified Face Analysis by Iterative Multi-Output Random Forests</papertitle>,
        <br>Xiaowei Zhao, Tae-Kyun Kim, Wenhan Luo,<br>
        <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, USA, 2014.</i>
        <br>
        [<a href="https://ieeexplore.ieee.org/document/6909624" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>

  </tbody>
</table>



<table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <p></p>
    <tr style="padding:0px;text-align: justify;">
      <td style="padding:0px">
        <papertitle>Generic Object Crowd Tracking by Multi-Task Learning</papertitle>,
        Wenhan Luo, Tae-Kyun Kim,
        <i>Proc. of British Machine Vision Conference (BMVC), Bristol, UK, 2013.</i>
        [<a
          href="https://www.researchgate.net/publication/269250315_Generic_Object_Crowd_Tracking_by_Multi-Task_Learning"
          target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>



    <tr style="padding:0px;text-align:justify">
      <td style="padding:0px">
        <papertitle>Active Contour-Based Visual Tracking by Integrating Colors, Shapes and Motions</papertitle>,
        Weiming Hu, Xue Zhou, Wei Li, Wenhan Luo, Xiaoqin Zhang, Steve Maybank,
        <i>IEEE Trans. on Image Processing (TIP), vol. 22, no. 5, pp. 1778-1792, 2013.</i>
        [<a href="https://ieeexplore.ieee.org/document/6392946" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>



    <tr style="padding:0px;text-align:justify">
      <td style="padding:0px">
        <papertitle>Single and Multiple Object Tracking Using Log-Euclidean Riemannian Subspace and Block-Division Appearance Model</papertitle>,
        Weiming Hu, Xi Li, Wenhan Luo, Xiaoqin Zhang, Steve Maybank, Zhongfei Zhang,
        <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 34, no. 12, pp. 2420-2440,
          2012.</i>
        [<a href="http://ieeexplore.ieee.org/document/6143947/" target="_blank">PDF</a>]
        <p></p>
      </td>
    </tr>
   



    <tr style="padding:0px;text-align:justify">
      <td style="padding:0px">
    
        <papertitle>Robust Visual Tracking via Transfer Learning</papertitle>,
        Wenhan Luo, Xi Li, Wei Li, Weiming Hu,
        <i>IEEE International Conference on Image Processing (ICIP), 2011.</i>
        <p></p>
      </td>
    </tr>



    <tr style="padding:0px;text-align:justify">
      <td style="padding:0px">
        <papertitle>Efficient Block-division Model for Robust Multiple Object Tracking</papertitle>,
        Wenhan Luo, Xiaoqin Zhang, Yang Liu, Xi Li, Weiming Hu, Wei Li,
        <i>IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP), 2011.</i>
        <p></p>
      </td>
    </tr>


  </tbody>
</table>


<table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <br>
    <tr style="padding:0px">
      <td style="padding:0px">
            <div class="section">
              <h2>Tech Report</h2>
      </td>

    </tr>

    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/stylemaster.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>StyleMaster: Stylize Your Video with Artistic Generation and Translation</papertitle>,
                  <br>Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, Wenhan Luo,<br>
                  <i>arXiv:2412.07744, 2024.</i>
                  <br>
                  [<a href="https://arxiv.org/abs/2412.07744" target="_blank">arXiv</a>]
                  [<a href="https://github.com/KwaiVGI/StyleMaster" target="_blank">Github</a>]
                  [<a href="https://zixuan-ye.github.io/stylemaster/" target="_blank">Project Page</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/KwaiVGI/StyleMaster?style=social">
                  <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/freecure.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Foundation Cures Personalization: Recovering Facial Personalized Models' Prompt Consistency</papertitle>,
                  <br>Yiyang Cai, Zhengkai Jiang, Yulong Liu, Chunyang Jiang, Wei Xue, Wenhan Luo, Yike Guo,<br>
                  <i>arXiv:2411.15277, 2024.</i>
                  <br>
                  [<a href="https://arxiv.org/abs/2411.15277" target="_blank">arXiv</a>]
         <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/pose_animation.gif' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control</papertitle>,
        <br>Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo,<br>
        <i>arXiv:2406.03035, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2406.03035" target="_blank">arXiv</a>]
        <p></p>   
      </td>
    </tr>




    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/promptrr.png' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>PromptRR: Diffusion Models as Prompt Generators for Single Image Reflection Removal</papertitle>,
        <br>Tao Wang, Wanglong Lu, Kaihao Zhang, Wenhan Luo, Tae-Kyun Kim, Tong Lu, Hongdong Li, Ming-Hsuan Yang,<br>
        <i>arXiv:2402.02374, 2024.</i>
        <br>
        [<a href="https://arxiv.org/abs/2402.02374" target="_blank">arXiv</a>]
        [<a href="https://github.com/TaoWangzj/PromptRR" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TaoWangzj/PromptRR?style=social">
        <p></p> 
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/lldiffusion.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement</papertitle>,
        <br>Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tong Lu, Tae-Kyun Kim, Wei Liu,
        Hongdong Li,<br>
        <i>arXiv:2307.14659, 2023.</i>
        <br>
        [<a href="https://arxiv.org/abs/2307.14659" target="_blank">arXiv</a>]
        [<a href="https://github.com/TaoWangzj/LLDiffusion" target="_blank">Code</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TaoWangzj/LLDiffusion?style=social">
        <p></p> 
      </td>
    </tr>



    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/face_survey.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>A Survey of Deep Face Restoration: Denoise, Super-Resolution, Deblur, Artifact Removal</papertitle>,
        <br>Tao Wang, Kaihao Zhang, Xuanxi Chen, Wenhan Luo, Jiankang Deng, Tong Lu, Xiaochun Cao, Wei Liu,
        Hongdong Li, Stefanos Zafeiriou,<br>
        <i>arXiv:2211.02831, 2022.</i>
        <br>
        [<a href="https://arxiv.org/abs/2211.02831" target="_blank">arXiv</a>]
        [<a href="https://github.com/TaoWangzj/Awesome-Face-Restoration" target="_blank">Project Page</a>]
        <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TaoWangzj/Awesome-Face-Restoration?style=social">
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding-top: 10px; padding-bottom: 10px;width:35%;vertical-align:middle;text-align:center">
        <img src='./multimedia/WSTG.jpg' style="max-width: 100%; width: 100%;">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle>Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of Sentence in Video</papertitle>,
        <br>Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, Kwan-Yee K Wong,<br>
        <i>arXiv: 2001.09308, 2020.</i>
        <br>
        [<a href="https://arxiv.org/abs/2001.09308" target="_blank">arXiv</a>]
        <p></p>
      </td>
    </tr>

  </tbody>
</table>



<table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

<tr style="padding:0px">
  <td style="padding:0px">


          <br>
          <div class="section">
            <h2>Thesis</h2>
            <div class="research">

              <papertitle>Generic Multiple Object Tracking</papertitle>, Dept. of Electrical and Electronic Engineering, Imperial College London, 2016.
              [<a href="Luo-W-2016-PhD-Thesis.pdf" target="_blank">PDF</a>]
              <p></p>
            </div>
      <br><br>
  </td>
</tr>
  </tbody>
</table>
  

</body>

</html>